{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-0",
   "metadata": {},
   "source": [
    "# Agentic RAG From Scratch: Building with LangGraph and Open-Source Models\n",
    "\n",
    "In this notebook, we'll look under the hood of `create_agent` and build an agentic RAG application **from scratch** using LangGraph's low-level primitives and locally-hosted open-source models.\n",
    "\n",
    "**Learning Objectives:**\n",
    "- Understand LangGraph's core constructs: StateGraph, nodes, edges, and conditional routing\n",
    "- Build a ReAct agent from scratch without high-level abstractions\n",
    "- Use Ollama to run open-source models locally (gpt-oss:20b + embeddinggemma)\n",
    "- Transition from `aimakerspace` utilities to the LangChain ecosystem\n",
    "\n",
    "## Table of Contents:\n",
    "\n",
    "- **Breakout Room #1:** LangGraph Fundamentals & Building Agents from Scratch\n",
    "  - Task 1: Dependencies & Ollama Setup\n",
    "  - Task 2: LangGraph Core Concepts (StateGraph, Nodes, Edges)\n",
    "  - Task 3: Building a ReAct Agent from Scratch\n",
    "  - Task 4: Adding Tools to Your Agent\n",
    "  - Question #1 & Question #2\n",
    "  - Activity #1: Implement a Custom Routing Function\n",
    "\n",
    "- **Breakout Room #2:** Agentic RAG with Local Models\n",
    "  - Task 5: Loading & Chunking with LangChain\n",
    "  - Task 6: Setting up Qdrant with Local Embeddings\n",
    "  - Task 7: Creating a RAG Tool\n",
    "  - Task 8: Building Agentic RAG from Scratch\n",
    "  - Question #3 & Question #4\n",
    "  - Activity #2: Extend the Agent with Memory"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-1",
   "metadata": {},
   "source": [
    "---\n",
    "# Breakout Room #1\n",
    "## LangGraph Fundamentals & Building Agents from Scratch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-2",
   "metadata": {},
   "source": [
    "## Task 1: Dependencies & Ollama Setup\n",
    "\n",
    "Before we begin, make sure you have:\n",
    "\n",
    "1. **Ollama installed** - Download from [ollama.com](https://ollama.com/)\n",
    "2. **Ollama running** - Start with `ollama serve` in a terminal\n",
    "3. **Models pulled** - Run these commands:\n",
    "\n",
    "```bash\n",
    "# Chat model for reasoning and generation (~12GB)\n",
    "ollama pull gpt-oss:20b\n",
    "\n",
    "# Embedding model for RAG (~622MB)\n",
    "ollama pull embeddinggemma\n",
    "```\n",
    "\n",
    "> **Note**: If you don't have enough RAM/VRAM for `gpt-oss:20b` (requires 16GB+ VRAM or 24GB+ RAM), you can substitute with `llama3.2:3b` or another smaller model.\n",
    "\n",
    "**ðŸ“š Documentation:**\n",
    "- [Ollama Installation Guide](https://ollama.com/download)\n",
    "- [gpt-oss Model Card](https://ollama.com/library/gpt-oss)\n",
    "- [EmbeddingGemma Model Card](https://ollama.com/library/embeddinggemma)\n",
    "- [langchain-ollama Integration](https://python.langchain.com/docs/integrations/providers/ollama/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cell-4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Core imports we'll use throughout the notebook\n",
    "import os\n",
    "import getpass\n",
    "import json\n",
    "from uuid import uuid4\n",
    "from typing import Annotated, TypedDict, Literal\n",
    "\n",
    "import nest_asyncio\n",
    "nest_asyncio.apply()  # Required for async operations in Jupyter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cell-5",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 7\u001b[39m\n\u001b[32m      5\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m      6\u001b[39m     test_llm = ChatOllama(model=\u001b[33m\"\u001b[39m\u001b[33mgpt-oss:20b\u001b[39m\u001b[33m\"\u001b[39m, temperature=\u001b[32m0\u001b[39m)\n\u001b[32m----> \u001b[39m\u001b[32m7\u001b[39m     test_response = \u001b[43mtest_llm\u001b[49m\u001b[43m.\u001b[49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mSay \u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mOllama is working!\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[33;43m in exactly 3 words.\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m      8\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mChat Model Test: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtest_response.content\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     10\u001b[39m     test_embeddings = OllamaEmbeddings(model=\u001b[33m\"\u001b[39m\u001b[33membeddinggemma\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/aimakers/github/bootcamp/AIE9/04_Agentic_RAG_From_Scratch/.venv/lib/python3.14/site-packages/langchain_core/language_models/chat_models.py:402\u001b[39m, in \u001b[36mBaseChatModel.invoke\u001b[39m\u001b[34m(self, input, config, stop, **kwargs)\u001b[39m\n\u001b[32m    388\u001b[39m \u001b[38;5;129m@override\u001b[39m\n\u001b[32m    389\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34minvoke\u001b[39m(\n\u001b[32m    390\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m    395\u001b[39m     **kwargs: Any,\n\u001b[32m    396\u001b[39m ) -> AIMessage:\n\u001b[32m    397\u001b[39m     config = ensure_config(config)\n\u001b[32m    398\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(\n\u001b[32m    399\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mAIMessage\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    400\u001b[39m         cast(\n\u001b[32m    401\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mChatGeneration\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m--> \u001b[39m\u001b[32m402\u001b[39m             \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgenerate_prompt\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    403\u001b[39m \u001b[43m                \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_convert_input\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    404\u001b[39m \u001b[43m                \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    405\u001b[39m \u001b[43m                \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcallbacks\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    406\u001b[39m \u001b[43m                \u001b[49m\u001b[43mtags\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtags\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    407\u001b[39m \u001b[43m                \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmetadata\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    408\u001b[39m \u001b[43m                \u001b[49m\u001b[43mrun_name\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mrun_name\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    409\u001b[39m \u001b[43m                \u001b[49m\u001b[43mrun_id\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpop\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mrun_id\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    410\u001b[39m \u001b[43m                \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    411\u001b[39m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m.generations[\u001b[32m0\u001b[39m][\u001b[32m0\u001b[39m],\n\u001b[32m    412\u001b[39m         ).message,\n\u001b[32m    413\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/aimakers/github/bootcamp/AIE9/04_Agentic_RAG_From_Scratch/.venv/lib/python3.14/site-packages/langchain_core/language_models/chat_models.py:1121\u001b[39m, in \u001b[36mBaseChatModel.generate_prompt\u001b[39m\u001b[34m(self, prompts, stop, callbacks, **kwargs)\u001b[39m\n\u001b[32m   1112\u001b[39m \u001b[38;5;129m@override\u001b[39m\n\u001b[32m   1113\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mgenerate_prompt\u001b[39m(\n\u001b[32m   1114\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1118\u001b[39m     **kwargs: Any,\n\u001b[32m   1119\u001b[39m ) -> LLMResult:\n\u001b[32m   1120\u001b[39m     prompt_messages = [p.to_messages() \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m prompts]\n\u001b[32m-> \u001b[39m\u001b[32m1121\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompt_messages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/aimakers/github/bootcamp/AIE9/04_Agentic_RAG_From_Scratch/.venv/lib/python3.14/site-packages/langchain_core/language_models/chat_models.py:931\u001b[39m, in \u001b[36mBaseChatModel.generate\u001b[39m\u001b[34m(self, messages, stop, callbacks, tags, metadata, run_name, run_id, **kwargs)\u001b[39m\n\u001b[32m    928\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m i, m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(input_messages):\n\u001b[32m    929\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    930\u001b[39m         results.append(\n\u001b[32m--> \u001b[39m\u001b[32m931\u001b[39m             \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_generate_with_cache\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    932\u001b[39m \u001b[43m                \u001b[49m\u001b[43mm\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    933\u001b[39m \u001b[43m                \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    934\u001b[39m \u001b[43m                \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrun_managers\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mrun_managers\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    935\u001b[39m \u001b[43m                \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    936\u001b[39m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    937\u001b[39m         )\n\u001b[32m    938\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    939\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m run_managers:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/aimakers/github/bootcamp/AIE9/04_Agentic_RAG_From_Scratch/.venv/lib/python3.14/site-packages/langchain_core/language_models/chat_models.py:1233\u001b[39m, in \u001b[36mBaseChatModel._generate_with_cache\u001b[39m\u001b[34m(self, messages, stop, run_manager, **kwargs)\u001b[39m\n\u001b[32m   1231\u001b[39m     result = generate_from_stream(\u001b[38;5;28miter\u001b[39m(chunks))\n\u001b[32m   1232\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m inspect.signature(\u001b[38;5;28mself\u001b[39m._generate).parameters.get(\u001b[33m\"\u001b[39m\u001b[33mrun_manager\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m-> \u001b[39m\u001b[32m1233\u001b[39m     result = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_generate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1234\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m   1235\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1236\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1237\u001b[39m     result = \u001b[38;5;28mself\u001b[39m._generate(messages, stop=stop, **kwargs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/aimakers/github/bootcamp/AIE9/04_Agentic_RAG_From_Scratch/.venv/lib/python3.14/site-packages/langchain_ollama/chat_models.py:1030\u001b[39m, in \u001b[36mChatOllama._generate\u001b[39m\u001b[34m(self, messages, stop, run_manager, **kwargs)\u001b[39m\n\u001b[32m   1023\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_generate\u001b[39m(\n\u001b[32m   1024\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   1025\u001b[39m     messages: \u001b[38;5;28mlist\u001b[39m[BaseMessage],\n\u001b[32m   (...)\u001b[39m\u001b[32m   1028\u001b[39m     **kwargs: Any,\n\u001b[32m   1029\u001b[39m ) -> ChatResult:\n\u001b[32m-> \u001b[39m\u001b[32m1030\u001b[39m     final_chunk = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_chat_stream_with_aggregation\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1031\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m   1032\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1033\u001b[39m     generation_info = final_chunk.generation_info\n\u001b[32m   1034\u001b[39m     chat_generation = ChatGeneration(\n\u001b[32m   1035\u001b[39m         message=AIMessage(\n\u001b[32m   1036\u001b[39m             content=final_chunk.text,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1043\u001b[39m         generation_info=generation_info,\n\u001b[32m   1044\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/aimakers/github/bootcamp/AIE9/04_Agentic_RAG_From_Scratch/.venv/lib/python3.14/site-packages/langchain_ollama/chat_models.py:965\u001b[39m, in \u001b[36mChatOllama._chat_stream_with_aggregation\u001b[39m\u001b[34m(self, messages, stop, run_manager, verbose, **kwargs)\u001b[39m\n\u001b[32m    956\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_chat_stream_with_aggregation\u001b[39m(\n\u001b[32m    957\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m    958\u001b[39m     messages: \u001b[38;5;28mlist\u001b[39m[BaseMessage],\n\u001b[32m   (...)\u001b[39m\u001b[32m    962\u001b[39m     **kwargs: Any,\n\u001b[32m    963\u001b[39m ) -> ChatGenerationChunk:\n\u001b[32m    964\u001b[39m     final_chunk = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m965\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mchunk\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_iterate_over_stream\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m    966\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mfinal_chunk\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m:\u001b[49m\n\u001b[32m    967\u001b[39m \u001b[43m            \u001b[49m\u001b[43mfinal_chunk\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mchunk\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/aimakers/github/bootcamp/AIE9/04_Agentic_RAG_From_Scratch/.venv/lib/python3.14/site-packages/langchain_ollama/chat_models.py:1054\u001b[39m, in \u001b[36mChatOllama._iterate_over_stream\u001b[39m\u001b[34m(self, messages, stop, **kwargs)\u001b[39m\n\u001b[32m   1047\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_iterate_over_stream\u001b[39m(\n\u001b[32m   1048\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   1049\u001b[39m     messages: \u001b[38;5;28mlist\u001b[39m[BaseMessage],\n\u001b[32m   1050\u001b[39m     stop: \u001b[38;5;28mlist\u001b[39m[\u001b[38;5;28mstr\u001b[39m] | \u001b[38;5;28;01mNone\u001b[39;00m = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m   1051\u001b[39m     **kwargs: Any,\n\u001b[32m   1052\u001b[39m ) -> Iterator[ChatGenerationChunk]:\n\u001b[32m   1053\u001b[39m     reasoning = kwargs.get(\u001b[33m\"\u001b[39m\u001b[33mreasoning\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28mself\u001b[39m.reasoning)\n\u001b[32m-> \u001b[39m\u001b[32m1054\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mstream_resp\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_create_chat_stream\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m   1055\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43misinstance\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mstream_resp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m   1056\u001b[39m \u001b[43m            \u001b[49m\u001b[43mcontent\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1057\u001b[39m \u001b[43m                \u001b[49m\u001b[43mstream_resp\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmessage\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcontent\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[32m   1058\u001b[39m \u001b[43m                \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmessage\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mstream_resp\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mand\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcontent\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mstream_resp\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmessage\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[32m   1059\u001b[39m \u001b[43m                \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\n\u001b[32m   1060\u001b[39m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/aimakers/github/bootcamp/AIE9/04_Agentic_RAG_From_Scratch/.venv/lib/python3.14/site-packages/langchain_ollama/chat_models.py:952\u001b[39m, in \u001b[36mChatOllama._create_chat_stream\u001b[39m\u001b[34m(self, messages, stop, **kwargs)\u001b[39m\n\u001b[32m    950\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m chat_params[\u001b[33m\"\u001b[39m\u001b[33mstream\u001b[39m\u001b[33m\"\u001b[39m]:\n\u001b[32m    951\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._client:\n\u001b[32m--> \u001b[39m\u001b[32m952\u001b[39m         \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m._client.chat(**chat_params)\n\u001b[32m    953\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._client:\n\u001b[32m    954\u001b[39m     \u001b[38;5;28;01myield\u001b[39;00m \u001b[38;5;28mself\u001b[39m._client.chat(**chat_params)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/aimakers/github/bootcamp/AIE9/04_Agentic_RAG_From_Scratch/.venv/lib/python3.14/site-packages/ollama/_client.py:181\u001b[39m, in \u001b[36mClient._request.<locals>.inner\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m    178\u001b[39m   e.response.read()\n\u001b[32m    179\u001b[39m   \u001b[38;5;28;01mraise\u001b[39;00m ResponseError(e.response.text, e.response.status_code) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m181\u001b[39m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mline\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mr\u001b[49m\u001b[43m.\u001b[49m\u001b[43miter_lines\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m    182\u001b[39m \u001b[43m  \u001b[49m\u001b[43mpart\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mjson\u001b[49m\u001b[43m.\u001b[49m\u001b[43mloads\u001b[49m\u001b[43m(\u001b[49m\u001b[43mline\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    183\u001b[39m \u001b[43m  \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43merr\u001b[49m\u001b[43m \u001b[49m\u001b[43m:=\u001b[49m\u001b[43m \u001b[49m\u001b[43mpart\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43merror\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/aimakers/github/bootcamp/AIE9/04_Agentic_RAG_From_Scratch/.venv/lib/python3.14/site-packages/httpx/_models.py:929\u001b[39m, in \u001b[36mResponse.iter_lines\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    927\u001b[39m decoder = LineDecoder()\n\u001b[32m    928\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m request_context(request=\u001b[38;5;28mself\u001b[39m._request):\n\u001b[32m--> \u001b[39m\u001b[32m929\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtext\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43miter_text\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m    930\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mline\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mdecoder\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m    931\u001b[39m \u001b[43m            \u001b[49m\u001b[38;5;28;43;01myield\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mline\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/aimakers/github/bootcamp/AIE9/04_Agentic_RAG_From_Scratch/.venv/lib/python3.14/site-packages/httpx/_models.py:916\u001b[39m, in \u001b[36mResponse.iter_text\u001b[39m\u001b[34m(self, chunk_size)\u001b[39m\n\u001b[32m    914\u001b[39m chunker = TextChunker(chunk_size=chunk_size)\n\u001b[32m    915\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m request_context(request=\u001b[38;5;28mself\u001b[39m._request):\n\u001b[32m--> \u001b[39m\u001b[32m916\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mbyte_content\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43miter_bytes\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m    917\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtext_content\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mdecoder\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbyte_content\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    918\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mchunk\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mchunker\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext_content\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/aimakers/github/bootcamp/AIE9/04_Agentic_RAG_From_Scratch/.venv/lib/python3.14/site-packages/httpx/_models.py:897\u001b[39m, in \u001b[36mResponse.iter_bytes\u001b[39m\u001b[34m(self, chunk_size)\u001b[39m\n\u001b[32m    895\u001b[39m chunker = ByteChunker(chunk_size=chunk_size)\n\u001b[32m    896\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m request_context(request=\u001b[38;5;28mself\u001b[39m._request):\n\u001b[32m--> \u001b[39m\u001b[32m897\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mraw_bytes\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43miter_raw\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m    898\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdecoded\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mdecoder\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mraw_bytes\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    899\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mchunk\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mchunker\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdecoded\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/aimakers/github/bootcamp/AIE9/04_Agentic_RAG_From_Scratch/.venv/lib/python3.14/site-packages/httpx/_models.py:951\u001b[39m, in \u001b[36mResponse.iter_raw\u001b[39m\u001b[34m(self, chunk_size)\u001b[39m\n\u001b[32m    948\u001b[39m chunker = ByteChunker(chunk_size=chunk_size)\n\u001b[32m    950\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m request_context(request=\u001b[38;5;28mself\u001b[39m._request):\n\u001b[32m--> \u001b[39m\u001b[32m951\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mraw_stream_bytes\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mstream\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m    952\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_num_bytes_downloaded\u001b[49m\u001b[43m \u001b[49m\u001b[43m+\u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mraw_stream_bytes\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    953\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mchunk\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mchunker\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mraw_stream_bytes\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/aimakers/github/bootcamp/AIE9/04_Agentic_RAG_From_Scratch/.venv/lib/python3.14/site-packages/httpx/_client.py:153\u001b[39m, in \u001b[36mBoundSyncStream.__iter__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    152\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__iter__\u001b[39m(\u001b[38;5;28mself\u001b[39m) -> typing.Iterator[\u001b[38;5;28mbytes\u001b[39m]:\n\u001b[32m--> \u001b[39m\u001b[32m153\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mchunk\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_stream\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m    154\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01myield\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mchunk\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/aimakers/github/bootcamp/AIE9/04_Agentic_RAG_From_Scratch/.venv/lib/python3.14/site-packages/httpx/_transports/default.py:127\u001b[39m, in \u001b[36mResponseStream.__iter__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    125\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__iter__\u001b[39m(\u001b[38;5;28mself\u001b[39m) -> typing.Iterator[\u001b[38;5;28mbytes\u001b[39m]:\n\u001b[32m    126\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m map_httpcore_exceptions():\n\u001b[32m--> \u001b[39m\u001b[32m127\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mpart\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_httpcore_stream\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m    128\u001b[39m \u001b[43m            \u001b[49m\u001b[38;5;28;43;01myield\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mpart\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/aimakers/github/bootcamp/AIE9/04_Agentic_RAG_From_Scratch/.venv/lib/python3.14/site-packages/httpcore/_sync/connection_pool.py:407\u001b[39m, in \u001b[36mPoolByteStream.__iter__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    405\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[32m    406\u001b[39m     \u001b[38;5;28mself\u001b[39m.close()\n\u001b[32m--> \u001b[39m\u001b[32m407\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m exc \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/aimakers/github/bootcamp/AIE9/04_Agentic_RAG_From_Scratch/.venv/lib/python3.14/site-packages/httpcore/_sync/connection_pool.py:403\u001b[39m, in \u001b[36mPoolByteStream.__iter__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    401\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__iter__\u001b[39m(\u001b[38;5;28mself\u001b[39m) -> typing.Iterator[\u001b[38;5;28mbytes\u001b[39m]:\n\u001b[32m    402\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m403\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mpart\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_stream\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m    404\u001b[39m \u001b[43m            \u001b[49m\u001b[38;5;28;43;01myield\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mpart\u001b[49m\n\u001b[32m    405\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/aimakers/github/bootcamp/AIE9/04_Agentic_RAG_From_Scratch/.venv/lib/python3.14/site-packages/httpcore/_sync/http11.py:342\u001b[39m, in \u001b[36mHTTP11ConnectionByteStream.__iter__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    340\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m ShieldCancellation():\n\u001b[32m    341\u001b[39m     \u001b[38;5;28mself\u001b[39m.close()\n\u001b[32m--> \u001b[39m\u001b[32m342\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m exc\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/aimakers/github/bootcamp/AIE9/04_Agentic_RAG_From_Scratch/.venv/lib/python3.14/site-packages/httpcore/_sync/http11.py:334\u001b[39m, in \u001b[36mHTTP11ConnectionByteStream.__iter__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    332\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    333\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m Trace(\u001b[33m\"\u001b[39m\u001b[33mreceive_response_body\u001b[39m\u001b[33m\"\u001b[39m, logger, \u001b[38;5;28mself\u001b[39m._request, kwargs):\n\u001b[32m--> \u001b[39m\u001b[32m334\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mchunk\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_connection\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_receive_response_body\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m    335\u001b[39m \u001b[43m            \u001b[49m\u001b[38;5;28;43;01myield\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mchunk\u001b[49m\n\u001b[32m    336\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[32m    337\u001b[39m     \u001b[38;5;66;03m# If we get an exception while streaming the response,\u001b[39;00m\n\u001b[32m    338\u001b[39m     \u001b[38;5;66;03m# we want to close the response (and possibly the connection)\u001b[39;00m\n\u001b[32m    339\u001b[39m     \u001b[38;5;66;03m# before raising that exception.\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/aimakers/github/bootcamp/AIE9/04_Agentic_RAG_From_Scratch/.venv/lib/python3.14/site-packages/httpcore/_sync/http11.py:203\u001b[39m, in \u001b[36mHTTP11Connection._receive_response_body\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m    200\u001b[39m timeout = timeouts.get(\u001b[33m\"\u001b[39m\u001b[33mread\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[32m    202\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m203\u001b[39m     event = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_receive_event\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    204\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(event, h11.Data):\n\u001b[32m    205\u001b[39m         \u001b[38;5;28;01myield\u001b[39;00m \u001b[38;5;28mbytes\u001b[39m(event.data)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/aimakers/github/bootcamp/AIE9/04_Agentic_RAG_From_Scratch/.venv/lib/python3.14/site-packages/httpcore/_sync/http11.py:217\u001b[39m, in \u001b[36mHTTP11Connection._receive_event\u001b[39m\u001b[34m(self, timeout)\u001b[39m\n\u001b[32m    214\u001b[39m     event = \u001b[38;5;28mself\u001b[39m._h11_state.next_event()\n\u001b[32m    216\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m event \u001b[38;5;129;01mis\u001b[39;00m h11.NEED_DATA:\n\u001b[32m--> \u001b[39m\u001b[32m217\u001b[39m     data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_network_stream\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    218\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mREAD_NUM_BYTES\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\n\u001b[32m    219\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    221\u001b[39m     \u001b[38;5;66;03m# If we feed this case through h11 we'll raise an exception like:\u001b[39;00m\n\u001b[32m    222\u001b[39m     \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[32m    223\u001b[39m     \u001b[38;5;66;03m#     httpcore.RemoteProtocolError: can't handle event type\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    227\u001b[39m     \u001b[38;5;66;03m# perspective. Instead we handle this case distinctly and treat\u001b[39;00m\n\u001b[32m    228\u001b[39m     \u001b[38;5;66;03m# it as a ConnectError.\u001b[39;00m\n\u001b[32m    229\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m data == \u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._h11_state.their_state == h11.SEND_RESPONSE:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/aimakers/github/bootcamp/AIE9/04_Agentic_RAG_From_Scratch/.venv/lib/python3.14/site-packages/httpcore/_backends/sync.py:128\u001b[39m, in \u001b[36mSyncStream.read\u001b[39m\u001b[34m(self, max_bytes, timeout)\u001b[39m\n\u001b[32m    126\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m map_exceptions(exc_map):\n\u001b[32m    127\u001b[39m     \u001b[38;5;28mself\u001b[39m._sock.settimeout(timeout)\n\u001b[32m--> \u001b[39m\u001b[32m128\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_sock\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrecv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmax_bytes\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# Verify Ollama is running and models are available\n",
    "from langchain_ollama import ChatOllama, OllamaEmbeddings\n",
    "\n",
    "# Test connection to Ollama\n",
    "try:\n",
    "    test_llm = ChatOllama(model=\"gpt-oss:20b\", temperature=0)\n",
    "    test_response = test_llm.invoke(\"Say 'Ollama is working!' in exactly 3 words.\")\n",
    "    print(f\"Chat Model Test: {test_response.content}\")\n",
    "    \n",
    "    test_embeddings = OllamaEmbeddings(model=\"embeddinggemma\")\n",
    "    test_vector = test_embeddings.embed_query(\"test\")\n",
    "    print(f\"Embedding Model Test: Vector dimension = {len(test_vector)}\")\n",
    "    print(\"\\nOllama is ready!\")\n",
    "except Exception as e:\n",
    "    print(f\"Error connecting to Ollama: {e}\")\n",
    "    print(\"\\nMake sure:\")\n",
    "    print(\"1. Ollama is installed: https://ollama.com/\")\n",
    "    print(\"2. Ollama is running: 'ollama serve'\")\n",
    "    print(\"3. Models are pulled: 'ollama pull gpt-oss:20b' and 'ollama pull embeddinggemma'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-8",
   "metadata": {},
   "source": [
    "## Task 2: LangGraph Core Concepts\n",
    "\n",
    "In Session 3, we used `create_agent` which abstracts away the complexity. Now let's understand what's happening under the hood!\n",
    "\n",
    "### LangGraph models workflows as **graphs** with three key components:\n",
    "\n",
    "### 1. State\n",
    "A shared data structure that represents the current snapshot of your application:\n",
    "\n",
    "```python\n",
    "class AgentState(TypedDict):\n",
    "    messages: Annotated[list, add_messages]  # Conversation history\n",
    "```\n",
    "\n",
    "The `add_messages` **reducer** ensures new messages are appended (not replaced) when the state updates.\n",
    "\n",
    "### 2. Nodes\n",
    "Python functions that encode the logic of your agent:\n",
    "- Receive the current state\n",
    "- Perform computation or side-effects\n",
    "- Return an updated state\n",
    "\n",
    "### 3. Edges\n",
    "Functions that determine which node to execute next:\n",
    "- **Normal edges**: Always go to a specific node\n",
    "- **Conditional edges**: Choose the next node based on state\n",
    "\n",
    "**ðŸ“š Documentation:**\n",
    "- [LangGraph Low-Level Concepts](https://langchain-ai.github.io/langgraph/concepts/low_level/)\n",
    "- [LangGraph Quickstart](https://langchain-ai.github.io/langgraph/tutorials/introduction/)\n",
    "- [StateGraph API Reference](https://langchain-ai.github.io/langgraph/reference/graphs/#langgraph.graph.StateGraph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cell-9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Simple echo graph created!\n"
     ]
    }
   ],
   "source": [
    "# Let's build our first LangGraph workflow - a simple echo graph\n",
    "from langgraph.graph import StateGraph, START, END\n",
    "from langgraph.graph.message import add_messages\n",
    "from langchain_core.messages import HumanMessage, AIMessage\n",
    "\n",
    "# Step 1: Define the State\n",
    "class SimpleState(TypedDict):\n",
    "    messages: Annotated[list, add_messages]\n",
    "\n",
    "# Step 2: Define Nodes (functions that process state)\n",
    "def echo_node(state: SimpleState):\n",
    "    \"\"\"A simple node that echoes the last message.\"\"\"\n",
    "    last_message = state[\"messages\"][-1]\n",
    "    echo_response = AIMessage(content=f\"You said: {last_message.content}\")\n",
    "    return {\"messages\": [echo_response]}\n",
    "\n",
    "# Step 3: Build the Graph\n",
    "echo_graph = StateGraph(SimpleState)\n",
    "\n",
    "# Add nodes\n",
    "echo_graph.add_node(\"echo\", echo_node)\n",
    "\n",
    "# Add edges (START -> echo -> END)\n",
    "echo_graph.add_edge(START, \"echo\")\n",
    "echo_graph.add_edge(\"echo\", END)\n",
    "\n",
    "# Compile the graph\n",
    "echo_app = echo_graph.compile()\n",
    "\n",
    "print(\"Simple echo graph created!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cell-10",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Visualize the graph structure\n",
    "try:\n",
    "    from IPython.display import display, Image\n",
    "    display(Image(echo_app.get_graph().draw_mermaid_png()))\n",
    "except Exception as e:\n",
    "    print(f\"Could not display graph image: {e}\")\n",
    "    print(\"\\nGraph structure (ASCII):\")\n",
    "    print(echo_app.get_graph().draw_ascii())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cell-11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conversation:\n",
      "  [Human]: Hello, LangGraph!\n",
      "  [AI]: You said: Hello, LangGraph!\n"
     ]
    }
   ],
   "source": [
    "# Test the echo graph\n",
    "result = echo_app.invoke({\"messages\": [HumanMessage(content=\"Hello, LangGraph!\")]})\n",
    "\n",
    "print(\"Conversation:\")\n",
    "for msg in result[\"messages\"]:\n",
    "    role = \"Human\" if isinstance(msg, HumanMessage) else \"AI\"\n",
    "    print(f\"  [{role}]: {msg.content}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-12",
   "metadata": {},
   "source": [
    "## Task 3: Building a ReAct Agent from Scratch\n",
    "\n",
    "Now let's build something more sophisticated: a **ReAct agent** that can:\n",
    "1. **Reason** about what to do\n",
    "2. **Act** by calling tools\n",
    "3. **Observe** results\n",
    "4. **Repeat** until done\n",
    "\n",
    "This is exactly what `create_agent` does under the hood. Let's build it ourselves!\n",
    "\n",
    "### The Agent Loop Architecture\n",
    "\n",
    "```\n",
    "                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "                    â”‚    START     â”‚\n",
    "                    â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "                           â”‚\n",
    "                           â–¼\n",
    "                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "             â”Œâ”€â”€â”€â”€â”€â–ºâ”‚    agent     â”‚â—„â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "             â”‚      â”‚  (call LLM)  â”‚         â”‚\n",
    "             â”‚      â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜         â”‚\n",
    "             â”‚             â”‚                 â”‚\n",
    "             â”‚             â–¼                 â”‚\n",
    "             â”‚      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”‚\n",
    "             â”‚      â”‚ should_      â”‚         â”‚\n",
    "             â”‚      â”‚ continue?    â”‚         â”‚\n",
    "             â”‚      â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜         â”‚\n",
    "             â”‚             â”‚                 â”‚\n",
    "             â”‚    tool_calls?                â”‚\n",
    "             â”‚     â”‚           â”‚             â”‚\n",
    "             â”‚    YES         NO             â”‚\n",
    "             â”‚     â”‚           â”‚             â”‚\n",
    "             â”‚     â–¼           â–¼             â”‚\n",
    "             â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”         â”‚\n",
    "             â”‚ â”‚ tools  â”‚  â”‚  END  â”‚         â”‚\n",
    "             â””â”€â”¤(executeâ”‚  â””â”€â”€â”€â”€â”€â”€â”€â”˜         â”‚\n",
    "               â”‚ tools) â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "               â””â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "```\n",
    "\n",
    "**ðŸ“š Documentation:**\n",
    "- [How to create a ReAct agent from scratch](https://langchain-ai.github.io/langgraph/how-tos/react-agent-from-scratch/)\n",
    "- [ReAct Agent Conceptual Guide](https://langchain-ai.github.io/langgraph/concepts/agentic_concepts/#react-agent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cell-13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AgentState defined with messages field\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.messages import BaseMessage, SystemMessage, ToolMessage\n",
    "from langchain_core.tools import tool\n",
    "from langgraph.prebuilt import ToolNode\n",
    "\n",
    "# Step 1: Define the Agent State\n",
    "class AgentState(TypedDict):\n",
    "    \"\"\"The state of our agent - just a list of messages.\"\"\"\n",
    "    messages: Annotated[list[BaseMessage], add_messages]\n",
    "\n",
    "print(\"AgentState defined with messages field\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "cell-14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LLM initialized: gpt-oss:20b\n"
     ]
    }
   ],
   "source": [
    "# Step 2: Initialize our local LLM with Ollama\n",
    "llm = ChatOllama(\n",
    "    model=\"gpt-oss:20b\",\n",
    "    temperature=0,  # Deterministic for reproducibility\n",
    ")\n",
    "\n",
    "print(f\"LLM initialized: {llm.model}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-15",
   "metadata": {},
   "source": [
    "## Task 4: Adding Tools to Your Agent\n",
    "\n",
    "Tools are functions that the agent can call. We use the `@tool` decorator and **bind** them to the LLM.\n",
    "\n",
    "**ðŸ“š Documentation:**\n",
    "- [LangChain Tools Conceptual Guide](https://python.langchain.com/docs/concepts/tools/)\n",
    "- [@tool Decorator Reference](https://python.langchain.com/api_reference/core/tools/langchain_core.tools.convert.tool.html)\n",
    "- [ToolNode Prebuilt](https://langchain-ai.github.io/langgraph/reference/prebuilt/#langgraph.prebuilt.tool_node.ToolNode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "cell-16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tools defined and bound to LLM:\n",
      "  - calculate: Evaluate a mathematical expression. Use this for a...\n",
      "  - get_current_time: Get the current date and time. Use this when the u...\n"
     ]
    }
   ],
   "source": [
    "# Step 3: Define Tools\n",
    "@tool\n",
    "def calculate(expression: str) -> str:\n",
    "    \"\"\"Evaluate a mathematical expression. Use this for any math calculations.\n",
    "    \n",
    "    Args:\n",
    "        expression: A mathematical expression to evaluate (e.g., '2 + 2', '10 * 5')\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Using eval with restricted globals for safety\n",
    "        result = eval(expression, {\"__builtins__\": {}}, {})\n",
    "        return f\"The result of {expression} is {result}\"\n",
    "    except Exception as e:\n",
    "        return f\"Error evaluating expression: {e}\"\n",
    "\n",
    "@tool\n",
    "def get_current_time() -> str:\n",
    "    \"\"\"Get the current date and time. Use this when the user asks about the current time or date.\"\"\"\n",
    "    from datetime import datetime\n",
    "    return f\"The current date and time is: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\"\n",
    "\n",
    "# Create our tool list\n",
    "tools = [calculate, get_current_time]\n",
    "\n",
    "# Bind tools to the LLM - this tells the LLM about available tools\n",
    "llm_with_tools = llm.bind_tools(tools)\n",
    "\n",
    "print(\"Tools defined and bound to LLM:\")\n",
    "for t in tools:\n",
    "    print(f\"  - {t.name}: {t.description[:50]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Agent node defined\n"
     ]
    }
   ],
   "source": [
    "# Step 4: Define the Agent Node (calls the LLM)\n",
    "SYSTEM_PROMPT = \"\"\"You are a helpful assistant that can perform calculations and tell the time.\n",
    "Always use the available tools when appropriate.\n",
    "Be concise in your responses.\"\"\"\n",
    "\n",
    "def agent_node(state: AgentState):\n",
    "    \"\"\"The agent node - calls the LLM with the current conversation.\"\"\"\n",
    "    # Prepare messages with system prompt\n",
    "    messages = [SystemMessage(content=SYSTEM_PROMPT)] + state[\"messages\"]\n",
    "    \n",
    "    # Call the LLM\n",
    "    response = llm_with_tools.invoke(messages)\n",
    "\n",
    "    current_count = state.get(\"iteration_count\", 0)\n",
    "    new_count = current_count + 1\n",
    "    \n",
    "    # Return the response to be added to state\n",
    "    return {\"messages\": [response], \"iteration_count\": new_count}\n",
    "\n",
    "print(\"Agent node defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "cell-18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tool node created using ToolNode prebuilt\n"
     ]
    }
   ],
   "source": [
    "# Step 5: Define the Tool Node (executes tools)\n",
    "# We can use LangGraph's prebuilt ToolNode for convenience\n",
    "tool_node = ToolNode(tools)\n",
    "\n",
    "print(\"Tool node created using ToolNode prebuilt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conditional routing function defined\n"
     ]
    }
   ],
   "source": [
    "# Step 6: Define the Conditional Edge (routing logic)\n",
    "def should_continue(state: AgentState) -> Literal[\"tools\", \"end\"]:\n",
    "    \"\"\"Determine whether to call tools or end the conversation.\"\"\"\n",
    "    last_message = state[\"messages\"][-1]\n",
    "    \n",
    "    # If the LLM made tool calls, route to tools node\n",
    "    if hasattr(last_message, 'tool_calls') and last_message.tool_calls:\n",
    "        return \"tools\"\n",
    "    \n",
    "    # Otherwise, end the conversation\n",
    "    return \"end\"\n",
    "\n",
    "print(\"Conditional routing function defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "cell-20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ReAct agent built from scratch!\n"
     ]
    }
   ],
   "source": [
    "# Step 7: Build the Graph!\n",
    "workflow = StateGraph(AgentState)\n",
    "\n",
    "# Add nodes\n",
    "workflow.add_node(\"agent\", agent_node)\n",
    "workflow.add_node(\"tools\", tool_node)\n",
    "\n",
    "# Set the entry point\n",
    "workflow.add_edge(START, \"agent\")\n",
    "\n",
    "# Add conditional edge from agent\n",
    "workflow.add_conditional_edges(\n",
    "    \"agent\",\n",
    "    should_continue,\n",
    "    {\n",
    "        \"tools\": \"tools\",  # If should_continue returns \"tools\", go to tools node\n",
    "        \"end\": END         # If should_continue returns \"end\", finish\n",
    "    }\n",
    ")\n",
    "\n",
    "# Add edge from tools back to agent (the loop!)\n",
    "workflow.add_edge(\"tools\", \"agent\")\n",
    "\n",
    "# Compile the graph\n",
    "agent = workflow.compile()\n",
    "\n",
    "print(\"ReAct agent built from scratch!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "cell-21",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Visualize our agent\n",
    "try:\n",
    "    from IPython.display import display, Image\n",
    "    display(Image(agent.get_graph().draw_mermaid_png()))\n",
    "except Exception as e:\n",
    "    print(f\"Could not display graph image: {e}\")\n",
    "    print(\"\\nGraph structure (ASCII):\")\n",
    "    print(agent.get_graph().draw_ascii())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "cell-22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing our from-scratch agent:\n",
      "==================================================\n",
      "\n",
      "Conversation:\n",
      "  [HumanMessage]: What is 25 * 48?\n",
      "  [AIMessage]: [Tool calls: [{'name': 'calculate', 'args': {'expression': '25 * 48'}, 'id': 'f1508a51-06eb-486d-a8c7-43370fe71a5c', 'type': 'tool_call'}]]\n",
      "  [ToolMessage]: The result of 25 * 48 is 1200\n",
      "  [AIMessage]: 1200\n"
     ]
    }
   ],
   "source": [
    "# Test our agent!\n",
    "print(\"Testing our from-scratch agent:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "response = agent.invoke({\"messages\": [HumanMessage(content=\"What is 25 * 48?\")], \"iteration_count\": 0})\n",
    "\n",
    "print(\"\\nConversation:\")\n",
    "for msg in response[\"messages\"]:\n",
    "    msg_type = type(msg).__name__\n",
    "    content = msg.content if msg.content else f\"[Tool calls: {msg.tool_calls}]\" if hasattr(msg, 'tool_calls') and msg.tool_calls else \"[No content]\"\n",
    "    print(f\"  [{msg_type}]: {content[:200]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing with multiple tool calls:\n",
      "==================================================\n",
      "\n",
      "Final response:\n",
      "Itâ€™s 20:41:15 (8:41â€¯p.m.).  \n",
      "100 divided by the current hour (20) equals **5**.\n"
     ]
    }
   ],
   "source": [
    "# Test with multiple tools\n",
    "print(\"Testing with multiple tool calls:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "response = agent.invoke({\n",
    "    \"messages\": [HumanMessage(content=\"What time is it, and what is 100 divided by the current hour?\")],\n",
    "    \"iteration_count\": 0\n",
    "})\n",
    "\n",
    "print(\"\\nFinal response:\")\n",
    "print(response[\"messages\"][-1].content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "cell-24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Streaming agent execution:\n",
      "==================================================\n",
      "\n",
      "[Node: agent]\n",
      "  Tool calls: ['calculate']\n",
      "\n",
      "[Node: tools]\n",
      "  Content: The result of 0.15*200 is 30.0\n",
      "\n",
      "[Node: agent]\n",
      "  Content: 30.\n"
     ]
    }
   ],
   "source": [
    "# Stream the agent's execution to see it step by step\n",
    "print(\"Streaming agent execution:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "for chunk in agent.stream(\n",
    "    {\"messages\": [HumanMessage(content=\"Calculate 15% of 200\")]},\n",
    "    stream_mode=\"updates\"\n",
    "):\n",
    "    for node_name, values in chunk.items():\n",
    "        print(f\"\\n[Node: {node_name}]\")\n",
    "        if \"messages\" in values:\n",
    "            for msg in values[\"messages\"]:\n",
    "                if hasattr(msg, 'content') and msg.content:\n",
    "                    print(f\"  Content: {msg.content[:200]}\")\n",
    "                if hasattr(msg, 'tool_calls') and msg.tool_calls:\n",
    "                    print(f\"  Tool calls: {[tc['name'] for tc in msg.tool_calls]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-25",
   "metadata": {},
   "source": [
    "---\n",
    "## â“ Question #1:\n",
    "\n",
    "In our from-scratch agent, we defined a `should_continue` function that returns either `\"tools\"` or `\"end\"`. How does this compare to how `create_agent` handles the same decision? What additional logic might `create_agent` include that we didn't implement?\n",
    "\n",
    "##### Answer:\n",
    "The check in should_continue is just on if condition. The prebuilt 'create_agent' has this with other production-grade safety and flexibility layers. \n",
    "1. It tracks how many times the  agent has looped and can raise a GraphRecursionError to save some cost.\n",
    "2. Morden models often run multiple calls at once. create_agent is designed to handle a list of calls and handle them. The above implementation expects only one call.\n",
    "3. It often includes the logic to force LLM to follow specific output schema, making continue or end decision more deterministic.\n",
    "4. The prebuilt version injects \"system message\" updates or handle message trimming/summarization automatically if the convesation history becomes too long for context window. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-26",
   "metadata": {},
   "source": [
    "## â“ Question #2:\n",
    "\n",
    "We used `ToolNode` from `langgraph.prebuilt` to execute tools. Looking at the tool execution flow, what would happen if we wanted to add logging, error handling, or rate limiting to tool execution? How would building our own tool node give us more control?\n",
    "\n",
    "##### Answer:\n",
    "ToolNode from langgraph.prebuilt is a black box, that\n",
    "1. extracts tool calls from the last message in state\n",
    "2. executes each tool with its argument\n",
    "3. return tool result as messages\n",
    "\n",
    "No hooks to log before/after execution, timing or results\n",
    "Errors may propagate without custom handling or retries\n",
    "no way to throttle calls or enforce quotas\n",
    "no visibility into execution metrics\n",
    "\n",
    "Building our own tool node gives us more control:\n",
    "1. Pre-execution: Before calling tool log the tool call, check rate limit, validate input, check permission\n",
    "2. Execution: During tool calls wrap them in try/except, add timeout, track execution time\n",
    "3. Post-execution: After tool calls log results, transform output, update metrics, handle error graciously "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-27",
   "metadata": {},
   "source": [
    "---\n",
    "## ðŸ—ï¸ Activity #1: Implement a Custom Routing Function\n",
    "\n",
    "Extend the agent by implementing a **custom routing function** that adds more sophisticated logic.\n",
    "\n",
    "Ideas:\n",
    "- Add a maximum iteration limit to prevent infinite loops\n",
    "- Route to different nodes based on the type of tool being called\n",
    "- Add a \"thinking\" step before tool execution\n",
    "\n",
    "Requirements:\n",
    "1. Modify the `should_continue` function or create a new one\n",
    "2. Add any new nodes if needed\n",
    "3. Rebuild and test the agent\n",
    "\n",
    "**ðŸ“š Documentation:**\n",
    "- [Conditional Edges](https://langchain-ai.github.io/langgraph/concepts/low_level/#conditional-edges)\n",
    "- [How to create branches for parallel node execution](https://langchain-ai.github.io/langgraph/how-tos/branching/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-28",
   "metadata": {},
   "outputs": [],
   "source": [
    "### YOUR CODE HERE ###\n",
    "from langgraph.prebuilt import ToolNode\n",
    "\n",
    "# Example: Add iteration tracking to prevent infinite loops\n",
    "class AgentStateWithCounter(TypedDict):\n",
    "    messages: Annotated[list[BaseMessage], add_messages]\n",
    "    iteration_count: int\n",
    "\n",
    "def custom_should_continue(state: AgentStateWithCounter) -> Literal[\"custom_tools\", \"end\"]:\n",
    "    \"\"\"Custom routing with iteration limit.\"\"\"\n",
    "    # Your implementation here\n",
    "    last_message = state[\"messages\"][-1]\n",
    "    \n",
    "    count = state.get(\"iteration_count\")\n",
    "    print(\"Count: \", count)\n",
    "    # If the LLM made tool calls, route to tools node\n",
    "    if hasattr(last_message, 'tool_calls') and last_message.tool_calls:\n",
    "        return \"custom_tools\"\n",
    "    \n",
    "    # to avoid infinite loop, we limit the number of iterations\n",
    "    if int(count) > 0:\n",
    "        print(\"Ending conversation\")\n",
    "        return \"end\"\n",
    "\n",
    "    # Otherwise, end the conversation\n",
    "    return \"end\"\n",
    "\n",
    "# Build your custom agent\n",
    "def custom_agent_node(state: AgentStateWithCounter):\n",
    "    \"\"\"The agent node - calls the LLM with the current conversation.\"\"\"\n",
    "    # Prepare messages with system prompt\n",
    "    messages = [SystemMessage(content=SYSTEM_PROMPT)] + state[\"messages\"]\n",
    "    \n",
    "    # Call the LLM\n",
    "    response = llm_with_tools.invoke(messages)\n",
    "\n",
    "    current_count = state.get(\"iteration_count\")\n",
    "    new_count = current_count + 1\n",
    "    \n",
    "    # Return the response to be added to state\n",
    "    return {\"messages\": [response], \"iteration_count\": new_count}\n",
    "\n",
    "def custom_tool_node(state: AgentStateWithCounter):\n",
    "    # First, execute tools using the prebuilt ToolNode\n",
    "    tool_results = tool_node.invoke(state)\n",
    "    \n",
    "    # Then, increment the iteration counter\n",
    "    current_count = state.get(\"iteration_count\")\n",
    "    \n",
    "    # Return both tool results AND updated counter\n",
    "    return {\n",
    "        **tool_results,  # Include the tool execution results\n",
    "        \"iteration_count\": current_count + 1\n",
    "    }\n",
    "\n",
    "tools = [calculate, get_current_time]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "cell-29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LLM initialized: gpt-oss:20b\n",
      "Tools defined and bound to LLM:\n",
      "  - calculate: Evaluate a mathematical expression. Use this for a...\n",
      "  - get_current_time: Get the current date and time. Use this when the u...\n",
      "Custom agent built from scratch!\n",
      "Testing custom agent:\n",
      "==================================================\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAARQAAAERCAIAAAA138+dAAAQAElEQVR4nOydB1wT5xvH37uEhI0yRZAlCoq46xYHonVra61a98A96qq17lX3HlT/jmqHWmtd1VqrVavWAXWAKKigDAEFkT2S3P2fy0EIENAgSe649yvmc/fee+9dLu/vnvd93iWmaRphMBjtESMMBlMhsHgwmAqCxYPBVBAsHgymgmDxYDAVBIsHg6kgWDy6IjlOFvbv2+SXefJ8Wi5XKPIJCKQJ5h8iaNiCXVKMKLkyNkkjiglBIhopIAIiCJpWhhAkoinmCCGiaQXBJl6YDlImhdjUihKBTRGiFAV3wqTAtkfQkCxF06TqJlWJs4iNkUQqNjYlHT1MmnephjDlQuB2nsolPiLv8olXb1/lwXMViQmJMWlsIiJFtCxP7TlD7lVmWZERUsiUASRBUUwEUkRQClopnkLNFOZvUkxQ8sJEmMNMNEQSjHiU5xIiglYURBCJkUJeGJcsiMv81IWXVh4ougqLkRQEivJyFXnZCrmMlhiLnDyMe451RBhNYPFUGq/jZSe/i8/NlFezlzRqW823vSXiOZd/SY56mJWTIbevJf1shjPCFAeLp3I4viX+5YscF2/zPoE1UNUiNUl+ek98ZprCr799gzbmCFMIFk8lsGd+FBR4Ri52RVWXiJDMv48kOXma9g7EpbgCsHg+lH2Lnzu4GPccU9UMjkb2LnzRpJNV087Yl8CAxfNBBM2L8vAx6zrMAQkG0E91B6NPptREgodEmIqyb8lzlzrCUg4wZrlrSkLelWPJSPBg8VSQ3/+XBD7iHmOEpRyWcSvdoQkLCR4sngry/FHG6EVuSKi4epvvWxyNhA0WT0U4uCrGpqYxEiHB0mtcjfxcKvR6BhIwWDwVIT05/9NpQm80dKxtcud8ChIwWDxac2ZPoqmFkZER0ifz5s07efIk0p6AgID4+HikA/oG1szNViABg8WjNQnROc61jZF+CQ8PR9qTkJCQmpqKdIbEmDx/8BUSKlg8WpOfRzXuZI10w/Xr18ePH9+uXbt+/fotXrw4OZnxCDdv3vzly5fLly/v2LEj7GZmZgYFBY0YMYKNtmnTptzcXPZ0f3//n3/+edy4cXDKlStXevfuDYF9+/adNWsW0gHV7CWvYnOQUMHi0Y6Y8FySRPa1dFJoe/z48fTp0z/66KNjx47NnTs3MjJyyZIlSKko+Fy4cOHly5dh4/DhwwcOHBg2bNjmzZsh/oULF3bv3s2mAKXJ3377zcvLa8eOHW3btoUIEAjlvQ0bNiAd4ORhkpMl3JIbHs+jHS+js0kxgXTDvXv3jI2NR48eTZJkjRo16tev//Tp09LRhg4dChbG3d2d3b1///6NGzemTZuGmJEKhJWV1ezZs5FeqOludu+KDouFHAeLRzuyMuSkzjzUjRs3hgLYjBkzWrZs6efnV6tWLSh9lY4G5uXff/+FQh2YJrmcGbVjbV1UjATJIX1hYc2MrUNCBRfbtIPJKbSuLI+3t/fWrVvt7Oy2bdvWv3//SZMmgVUpHQ2OQjkNIpw4cSI4OHjUqFHqRyUSCdIXBClSDmcVKFg82mFsIqIUOnzXtmnTBuo2p0+fhtpOWloaWCHWtqigafrXX3/9/PPPQTxQtIOQjAyDtVRmvpUjAYPFox01XI3lcl2JJyQkBGovsAHGp1evXuAiA2GAu1k9jkwmy8nJsbe3Z3fz8/OvXr2KDERidK7uaoDcB4tHOzwbm8HLPztdJ/qBQho42Y4fPw6NM2FhYeBVAxU5OjpKpVJQy82bN6GQBr4ENze3U6dOxcXFvX37dtmyZVBTSk9Pz8rKKp0gxIRPcMdBakgHxD7NMpIINwth8WiNSEzc+kMn3VLAjQaFsfXr1wcEBAQGBpqZmUHdRixmnDrggrtz5w7YIjA7q1atAqfcgAEDoJGnRYsWU6ZMgd0uXbpAW1CJBJ2dnaGpBxqFoJqEdEDqq/watfTdXswd8GA4rTm6KQ7K+qOXuiHBs+3LJ0O/dqtur9+uSpwBWx6t6TasRla6oCvKLH8cTIQym2CVg3A7TwWwshVLTchjW+MHTHPSGIGiqM6dO2s8BPV7aKWBpszShzw8PPbt24d0AzS/guMOaXlL4DqHIh8qg6jQLN+2VkjA4GJbRXj1Iv/I5hdTN9UpK0Lp6gdLZmamubnm2ZugbqPyoemCCtwSiAo8FhoPXTryOiIkfeLa2kjAYPFUkCMb4uQy6ot5LkiQ7Jj9tNcYZ9d6wvUWIFznqTCfz3LOeCu/eUaIPbv2L3nu5G4icOUgLJ4PYcJqj/8uv0mOE1a3YjC5pJjsN9kJCR5cbPtQds5+5tffoUFbQcxDe3BlTDU7oz540lAlWDyVwM45z5xqm/WdUMUnDd235LnEWDx0Hp7xvQAsnsrh++UxuVnyFgE2TfyroPf2VNDL2CfZtRtafDxCiPPUlQUWT6Xx7+nUe1ffIAK51TPvPtyhCkxMFfs478bvr5Nf5ppagMFxMxK6g6AkWDyVzNXjyRHB6bk5CrERaWouMrMyMjUXk0ZIllfkV1AtZaVO0SJWJDSzKgeFMovDFVu8TSRmDhUsekUUrPcmNiLkMpogCZpil7AqSg0+YZdZ2YpZQ46JwF6auQStvALTpFt0lkRKymUoK12ek6HIyVHQctrC2sjvUzuXuiYIUwosHl1x7URKQnRORpqcliMFohX5Rc9Zle+RMm+z48kKM3Dhom/wnyTYfK+KTIiYhRPZXUZYyvxPimhKQahHK0qEYGMqU1deqkBUBUeV/9VWhpMYwzVJI2PSylri0dDcpzVejac8sHj4yowZMwYMGNCuXTuEMRC4bxtfkcvl7GgFjKHAT5+vYPEYHPz0+QoWj8HBT5+vgHj0PWE2pjhYPHwFWx6Dg58+X8HiMTj46fMVLB6Dg58+X5HJZFg8hgU/fb6CLY/BwU+fr2DxGBz89PkKFo/BwU+fr2DxGBz89PkKOAxwI6lhweLhK9jyGBz89HkJxQyXQySJJz8yJFg8vASbHS6AfwBegis8XACLh5dgy8MF8A/AS7B4uAD+AXgJFg8XwD8AL8Hi4QL4B+AleBgpF8Di4SXY8nAB/APwEmgkLWvNNozewOLhJWB2EhMTEcagYPHwEhAPlNwQxqBg8fASLB4ugMXDS7B4uAAWDy/B4uECWDy8BIuHC2Dx8BIsHi6AxcNLsHi4ABYPL8Hi4QJ4HC8vYVZTJEmFQoEwhgOLh69g42NwsHj4ChaPwcF1Hr6CxWNwsHj4ChaPwcHi4StYPAYHi4evYPEYHCwevoLFY3AImqYRhj80btxYJBKxvxp8QoMPfHbr1m316tUIo1+wq5pn+Pr6IuUs1QCoCD4dHBxGjhyJMHoHi4dngE5MTEzUQxo2bOjt7Y0wegeLh2f4+/t7eXmpdm1sbIYMGYIwhgCLh3+MHj3a3Nyc3QYhQS0IYQwBFg//aNu2bf369WHDyspq0KBBCGMghO5texWd//BOenamnFJQxQ4QBDizSBJRqmB4z1CIIBHNhhDwD5V4eHCUQAWnkCRBUTRBEjRFF6anSrvYYydFiFIUj8GGw9XpkoGQIEHSqW/SwsIempmaqpudonsrSoG5kMZfGBKHYPX4TMoQQBV9/aLbKBaNhEilT2ejqb6vxvuRGJNWNtJWPaqjKoGgxfP98pjsDLmRhFTIKIoqcZBmcpD6z0/QiGYVw+4q/0qcRVIEIgtOYcXGPGCiID5dLPGik1jxFA9EBZmvVCDskTStICjGT81KmIEiaEa5JX5MUnnXmn5hJnFa+Y2KgpSSoQquUiT1EhogKUSRytPVv1FBtGJvgGJfmcFICtIiFHLK2dO0V2ANxHOEK569i55b2kg/HumIMPolMx2d2xNTr6VF6578NkECFc/+pTF2jiYdPscz1hqMI2ueu/madRnM459AiA6DiODc/BwFVo5h8WltHR2WifiMEMUTeTdVaordjAamgZ+lLJ/O4bN8hJiHsjMoPPifC1AKOj0lH/EWIfaqlisUJR3TGEMArnwK8fiHwEMSMAZD6cfnsb8KiwdjMAjVBz8RonhETOMiHsVkeGjVBz8RongUJVrWMQaDILDl4RciESJFWDyGh+B5A70gLY+CcZIijKFher3iYhvfwMrhCAR2GPAOXGbjCDR2GGAwFUE5BoLHfVyEKB5mOBnu2sYBlEONcA8DXkHRNIV753AAprGNz20GwrQ8yvGYGEPDCIfPrdVCLL5QZYxM1g+/nTj67ZrFqGoRHf1s0JBeSEvgV8CNpBgtiIgIR1WOiMiKfCkCt/MIAYVC8cuxH78/uBu269fzHTlivK8vM21N957tRgwPHPT5cDba2nXLnj2L/C7oB9iOiXm+/0DQvfshNE37+DQcNHA4nDJjZuD9+//B0T///B2i1a3jDdE2b1kd+eSRSCR2c/OAlJs0bg4Rli6bRxBE61bt121YLhKJvL18lixec+LkL3APlpZW3br2mjB+OvGu0ufx347cvPnPo0dhEqm0UcOmY8ZMdqrpzB46dfrXo0cPpWekt2rVbsyoSWA3Fnyz0r9zNzj0x/nTcDQ6+qm7u2fnTl0//WQweyH2lrr4d1+9dklOTnb9+r4TAqfXq9cAvubBQ/+DCJ38m0+a+OVnA75AwkCIxTZC+6a53Xu2nTz5y7Kl6xfMX2ln5/DV11Mh05cTPz8/H3QCmX7N6m0b1u0Si8TfLPgyNzd388bdkNu6du3598VgUE5q6pspU0fZ29fY/d1PO7btr17NevmK+dnZ2Ui5CELYw/vw98uRc0E7D8HG9C/HUZTizKkrixetPvrLD7duXS//nkND723bvs7Hp9GyZevnfbUUrrVy1QL20KPHDzdt/rZDhy6Hvj/e0a/LshVfI+X81/D518U/1qxdCvf20w+nxo6ZfOzXn7bv3MCeBbf0MPzBhb/OBu06dO73a1KJlC1/jho5AV4fDg414EtppRwC923jHcxvpk1hAV7PkFlnTJ/3UfNWsNuyZdvs7KyUN8kuLm5lnRIb+wIyK7yzIRfCLmT3+w/+K70iCFgzsAmzZy2AfAm7c2YvGjCw28lTvwweNAIpFThl8mwjIyMrq2oe7p5yhRyyKYSDaapWrfqzqCdgNMq5bbAM+/cedXZ2YROXy2TzF3yZlp5mZWn1559nrK1tIDU41KaNH9i98PBQ9qyzZ080bNgEvixsV69uPWrEhLXrlw0dMhq2ISQnOxtu0tTUFLb9O38MJgikzu5WAGXdExfbeAWl5QismBfR8Ont7cPuQoZbtnRd+adAloX8DXkroEuPxo2aNWjQiC2MlSAq+mmdOt5s5gbMzMxqObtGRj5id52caoFy2G0TU1Mba1vViWamZpmZGeXfA9i9ly/jduzc8OhxWFZWFhv4NvUNiAeuCwZQdV2/9v7fH9yDmKGdFJi44cPGqRJp0uQjCHwQereDnz/s1nJxU0nF3NwCPjMy0issHuY1RuNG0ipNVjaT84ylxu9/ilQq3bJpdRelxQAAEABJREFUz+9nT0CxZ+++nTVrOo8cHhgQ0KNEtDcpyaAQ9RBjE5PsnGx2myzelEtq2bJ7/fqVBYtmfTFk1PjA6bVr1wkOuTX3qynsIRAelBVVMcGysRtg62QyGdww/KknBVa0YvfwDmh+u6oF286jRVHb1IR5s2ZnZ70zpoIqmlgECnUTJ8yAotF//90+98epVasXubp5sKW4opTNzHLzctVDoFzk7OSCKoMzZ38DFwXUW9hddUsllRpDKU61C0VQdsPY2BjMSNeAnn5KO6OipqMz0g28LrYJtp1Hi9/Mw6MOlHCg0sLuwrnz5k8/f/4MbEsk0pxCQ4GUVR12A9wJIBikzI5QqQBHGaSgKo+p8KpbH1xhssJ8DJWrFzHR7u61UWWQnp5mZ2uv2v3nn0uqbTB30c+fqXavX7+s2q5du25GZgYUMtm/Bj6NoLhob++AdAOvHQZCFA9jebR54UFVBKou4G0DPdy9FwwurJCQW1BnQMpK+ZWrFzMzmdnHDv2wNzn5FXsKZFxwW+8K2hwXHwuK+vGn/eAtgIyIlBkXBPPf3TtQFurd+9OsrMwNG1cmJSU+fx717epFUDjs0b0fqgw8a9e9E3wTbhguDZ4JNjAxKQE+27bp8OJF9E8/H4AXAcQBv5zqrHFjpoCWzp47CVUdCF+2/OuZsydAca78a0EdLyUl+dq1y6rXx3uCLQ/PYCyPli+86dO+aty4OeTymbMmMFlqyTrW1QbeMOvqNr37dgzo1iovLxccUGx88BDM/HL+XxfPDRvef/jIT0ND727cEATNOHCod89PoNA4Z+5kcJc5O9UCRxy0qEAzC7i24eiWzf8DraLKYPToSS1btFmwcGbXj1uDOMFb7e1Vf97X08AZ7de+c/9+A6HJqP+nAb+dODJ2LFMXYp0TUNLbHfTjgwd34dDsuZNA2yuWb4QqXPnXatWynW+DxgsXz7546TwSDEKcq/rHdTHZaYpBc9yRUAFbBIbO07MuuwvNPpMmj9jz3U+qEP3w/ZKnn053dnTTwhPDKYRoecBDKxILumdoaNi9ceOHbNm6JjExAVp4tmxZ7ePTEDxySL/gdh7+AW2VCnlVsLdQafn55wMaD4Fnb/vWfWWdCJ6AWTO/gSrc6LEDobmmebNWEybMIHBXcy0RqKu6agD+hk6dumo8JBa945ft1bM//CGDQiB+j4gXZg8DVDWwMLewUDbz8xQa8XsuFkH2bcOD4TgD7hjKM2iDDobDqOD7Gwz3bcMYDJrnjaRYPBhMBRHmeB4az3uI+XAE2bcNwNrhAHgkKf9QUDSFPQYcAPcwwGAEChYPBlNBhCgeYxORIhdhDI5YLBIRIsRbhOgwsLaVyPJxncfAZL5hmnkcXCWItwhRPJ0G2eXnynMyEcaA/Pt7kpmVEeIzAl1qo3ZDixPboxHGQLyJVyS9yB6+oHKmOjEUQhxJyvIkJOvSsSSHWmau3uakMaJkirJi0oRyqEvZnbGVk/0TlMZZlAh2giVCQ3c6ZSCJSKr0GjXsWdAaVfyiBcEkQamFM7enTJxEBKXm+aWVFyAJEjzzJVNQi8meThAkXTwaMzOk+j0zy1AVS4oJoYp/6+I3TLONOOrHxURmiuLFo4y0N7KJazwQzxGueICIO9k3zyXnZivk+e+YCJEgCbps8dDKHKi5xYJgc7Gm56wUj8aU2R4QpQ/RSkGUeTNEsZxKE8y/EpdmUygek9kpkSZEA4kWO1Ep2mJxGGUWX9AayjFUsfshULGXhkhMiI3IanaSz750QvxH0OKpFCIiImbPnn3w4MHq1asjPTJr1qy+ffv6+fmhyqN79+5ZWVnwRTw8PDp06NCwYUPYQJgywO08Fef+/fuNGjWCV/vp06eR3rGxsan4PLdlAOI5dOhQdnZ2bGzs9evX7ezsLC0tmzRpMnfuXIQpBbY8FWTv3r3h4eEbNmxAVYjHjx/PmTMnISFBFUIzS1BSLi4uJ0+eRJji4IVttQY0g5jpDusbVjmvX7/Oy8tDlYq3t7eTk5P6+xTsaq1atbByNILFowVyuTwwMDAxMRG2W7dujQzKwoULQ0NDUWXTp08fExMT1a6tre2pU6cQRhNYPO9LWlpaUlLShAkTOnfujDiALuo8QKdOnSBldhs8B6mpqfCtEUYTWDzvJj4+vnfv3lCAgSJN06ZNETdYuXIlFB1RZQNmB4wq1HPMzMwuXLhw69atMWPG6MLEVQGww+DdHDt2rG3bto6OjohLgEEAyyCR6KRvWEBAAChHtTtq1Khhw4ZxxORyB2x5yuSPP/4YP348bAwYMIBrykHKdp6oqCikG9SVA+zfvx+expEjRxBGDSweDbBL6j548OC7775DXMXBweGdixdUImvXro2Jidm+fTvCFIKLbSUBwYDHFtrXEaYUBw4ciI6OXrp0KcJgy1OCGzdukCTJC+WAx7z08tq6ZuTIkS1btpw0aRLCYPGoWLFiBXyCM23cuHGID0AOfvnyJdI7PXr0AAlBPRAJHiwehsmTJzdr1gwplxBFPMHe3l6fdR51WrRosX79+o4dO7KVQ8Ei6DpPXFzc1atXhwwZgjDak5WVxXYkdXV1RYJEuJYnIyNjypQp/HUMQJlNoVAgwwGtqPDqmTlzZnBwMBIkQrQ8d+7csbGxgWKPubk54i1du3Y9fPiwtbU1MjQTJkzo27cvWCEkMARneS5durRv3z4XFxdeKweoWbOmWMyJ4VhBQUHgpTx48CASGAKyPP/880/79u0jIyPr1tXrms8CYdu2bfn5+bNmzUKCQSiWZ86cOY8fP4aNKqOc+Ph4Tr34pk6dCsbwq6++QoKh6lue8PDw+vXrP3jwoGHDhqgK0a5du4sXLxrKW10WcEs//fTT3r17kQCoypYnLS2tZ8+eJMl8xyqmHMDR0ZEjdR51/P39p02b1rt3byQAqqblAR+uSCQKCwuzs7NzcHBAGP2SkJDQv3//c+fO6XlGIT1TBS3P7du327RpAxsNGjSowsqBFl7EVcAqXrt2beDAgeCeQVUXfVgeaIqmKArpnry8PFtb26NHj8LPhqo0crkc6jw3b95E3GbIkCGTJ09u27Ytqoroo9AMHkw9tIWDROETGg2rvHKQckYoaKpCnAecB9OnT3/9+nW/fv1QlUMflic1NVWn4qGV5ObmmpqagnhYDwGGO6xYscLe3j4wMBBVLXifzzIyMqBMCILRxVQynAVeFtDOg3jCggUL4HPVqlWoasFv8WRnZ0skEnCsIYGRnp4+fPhwxB/A7Hh7e8+YMQNVIXgpHjA158+f//jjj6E2xbVWQr1Rq1YtxCs++eSTAQMGDB06FFUVeDnRO7x3dTTlEl+wsrI6cOAA4hvgIYSWt65du0ITUBUoL/DJ8oCdAa8AbFSrVk2ARTV1wAGjPh07j/Dy8jp8+DA0xLGzFvMaw4gnPDz8m2++ASM+ZsyY3bt3q0bznjp1avDgwbGxsePHj4dS2cSJE//880/2EGSXPXv2jBw5cvTo0QcPHtT/3BecAnLehAkTED8Bj+itW7fGjh3L94lIDSAecBPNnz8fbMimTZsWLVoUHR09Z84cVgxGRkaZmZk7d+6EmiVY9vbt20Mc0BI4l86ePXvhwgVocduyZUuNGjV+/PFHJGDA8NasWRPxmTNnzmzcuPHSpUuItxhAPH///bdYLAbZQJXX1dUVdPLs2bMbN26wR2Uy2RdffFGvXj2CILp06QKyefLkCWyDUWqvxMLCAgrNjRs3RgIGXh+7du1CPGf//v3g+IFSHOInBhAPlNmg4AtVXnbXwcHB0dExLCxMFQGOImVfG3awJxTYQEIvX75Ub1OvU6cOEjBgqKtAnQFYs2ZNXFwcFN0RDzGAeKBgFhIS8rEaUPdNTU1VRQA7k5OTo94pASpFsKu+bgyP5ojSBW/fvl25ciWqEsyePTsmJub27duIbxjAVQ31RR8fnxJtfJaWluq7UKYnSRLca+yuqakphKgvhAbqQgLG1tZ23bp1v/32W//+/RH/gVcnH4fGGEA87u7uFy9e9PX1VXVCe/HihZNTsbXF2WYclXjAFtnb2z969EgVgY8vqsoFbG+fPn1SUlJUa1HxF7A8vOjnWgIDFNugpZmiqKCgIHC4QXl379694HV9/vy5ehyQDXgO1EP8/PyuXbt29epV2D569Cg7IYHAAWucnJwM/hXEZ6D+9vr1aw4u4vJODCAecJeBcuDFOXXqVHD2P3jwABxunp6e6nFAPCVacqD9B2pH4GKCT2glYLvo4iUewLmyffv2O3fuIN4C5Q6ezjnK0SEJYHagqFaBMfrCHJIAPhgwQW5uboiHXL58Gdp81q9fj/gGR/MZtJZycHYLzgI+/aioKJ5O+wQVHp5aHo5mUCi2geUBCSHM+9G5c2doOOZjzRuKbTyd24ijlgcqPCUcBph3AkVWcCE8ffoU8YrY2FjeDa9g4W6xDZudCgAe/3/++WfHjh2IP+BiWyWDlVNhRo0alZGRAS4EXsxkn62Ep01VHLU8UGZTtZBitAUaAyIjI3lRfuNp8yiLPiyPpaWltg7xY8eOwesTXqJIS/DUOSxNmzZdtmwZuBD69OmDOAwWzzuowKhPX1/fnJwc7K3+EBYtWgR+F3ZqIcRVeC0ejj5WHx+f5s2bI8yHAW+fs2fPgi8YcRVwtWHxVDKhoaFsNzbMB9KrV68tW7ZwdsAzCBsX2yqZiIgIqO/6+fkhzAezceNGxFX428iDOGt5oM7D33WqucnWrVvVRxxygbS0NIIgVGOKeQdHxePl5dW6dWuEqTymTZu2YMECTumH12U2xFnxhIeH83peFW6yY8cOTq02xesyG+KseKKiorDDQEdMnjwZcQP+dsxh4ah46tev7+/vjzA6APwHc+bMQRwAxMNry8NRb5uHEoTRAVKpdN26dYgD8LqFFHHW8jx58uT8+fMIozNevXo1adIkZFCweHQCVCWxw0Cn2NvbL1682ICzDb5+/drc3Fx9Lj7ewdFiW4n5QDC6wMHBwYBLHfLd7CDOWh54rJ07d0YY3XP79m2DTH6AxaMroPnszJkzCKN7WrRoMX78+L/++gvpF7438iDOiufly5fYYaA3wLHZqVMnPY8+5O90bSo4Kh43N7fevXsjjL4QiUQnTpxYu3atemAFBiO+P1Wg2EbgSTcxKiIiIuRyuY+PD2x36dKFJEmQk46WQmrVqtX169d5vTwmRy1PfHz88ePHEUa/eHl5OTs7Z2RkdO3a9e3bt8nJyTqayBd+X/D18X1hWY6KB5rwzp07hzB6x8rKKiAg4M2bN0i5OAUYB6QDqoC3AHFWPE5OTp988gnC6B0oramm2AfxJCUlRUVFocqG711CWTgqHmj/7t69O8LolzZt2kBpTT0ESm4hISGosuF7l1AW7hbbjh49ijD6Baw9GAQLCwuKotgQhUJx5coVVNlUAVcb4mz3nNTU1FOnTg0cOBBhyubZgxxZXvEZvQnlf5x4jJ8AABAASURBVHUPKkkQdPGFjAhEUARNQqgqhDmFIMheHQLhLzIi4kFoaHz8y7S0t3n5+TmvrO5cigdFQVKIogkEZ1LFr8gkWXQJCKGVQcV8ucrQwsC8ZBs6o+bj4AzlrRLK66vfM4IrECRJU1SJb0fQ8A+hEi5iSIBiEi/pOCaU3710fKTpQakhNZG6N5Cid8EtV/W4cePA1cM83Ly8rKwsU1NT2M7Jyblw4QLCqPHDqpj0VBlBIkX+u38+giRoqlg0uiDPl4iHSuY+TfGIUllOmf1Ln6tU2XsEaji9dGrqsclSR+GWKGUiGqLThKbvilhllZH5xUbMXVa3kwyaU17ZkluWp0GDBocOHVLtsgPuof6DMGrs/jrKuoZJ99EuEh73SOY6GSmKy0cTDi5/MXxhmY4NbtV5hg4dCu0M6iFQ+G7WrBnCFALK8W5p022kI1aOTrGwEfWe6Gxpa7x/SZlTRnJLPDY2Nj169FAPAbMzePBghFHy56FXYqmoSSe+ztXEO/yHOMjyqeA/32o8yjlvG0hF3fj4+vrWr18fYZQkRufa2BsjjB6xsJI8Dc3SeIhz4gGvTq9evdgp3sEQDR8+HGEKycuTi40JhNEjpJjOz9K8SCEX23mg5uPk5ISUc+iA5UGYQuQyWlHY/I/RD/kyKl9WhlMOfQC5OejOueSE53nZmTJZPk0roH5fdBnWp6nu2SwdUgKCddXTqJPbt3LnfInYOOirKJJEJd39ypdvQSJKT35BmmW4OEViQiQiJSak1IRw8TZr+TGHJv7D8JcKigdqrs8fZcpyaVJEMllTKjIyFjEOfKpYYxzSmKHL8eITiG3TkiIJQqaFgZrUVhSollwZKRMi+C/Oz1VkpuUnxabcPp9samFUv4VF6168XM0PwxG0Fs+5/UnRDzMJEWFpZ+7kw8vMp8hXxIW9uXv57d0rbz8KsP6oKzZEmDIhxUzJReMh7cSze340lKBq+TpY2PO4lUEkEbk2tUPI7tXTtOC/UsP+TR+1mPc9fDG6Qo4oOaXxyPs6DF5G5+6Y9dTc1sy7gwuvlaOOvadVvU6uNCHaOesZ4gOkiOlrgzB6hGL75GnivcSTkaI4vi2uXge3mvWqYCXB4yNHu9q2O2bzQD+UApXopYYxIO8WT0x4zsHVzxsEuJNGVfadZ+du7t645k4+6AejZ0iSIEUVtTyn9yV4teD9uKV3YmojsalVLWhe5Y+axPAaaH2hFJqt/TvEAx4CC1tTsTm/J2p4TxzqVhNLxEc2xCHOQiCEcJ2HK5Qnniu/pMjltEsjOyQYPFs7vYrLTYrV6/R/WkEgXOfRK+W8r8oTT+jNVDvXakhgWFibntkTjzgJofrA6A2izCdepnhunEyBs+w8ONr7/V7oX7MXtszMqvzlad2aO2RnyDNSKcQ9aKYTB7Y87+bX44e7dG2JKgmC0NJhEH4n3ayaQHu/G0mNzh9MQJjiLF027+y5k0j3REc/GzSkF+IG8MKiKC0dBjlZCgcvayRIrBzM3yRyt9pjKCIiwpFeiIjU04U+EM3dc8L+zSRIZGIhQbrhecyDP//+X2xcuLlZ9Xpe7bp2GmtsbAbh12/+cuHKvomjdx08/HXSqyhHB0+/NoM/alrwEjrzx7bg+2elEtMmDbvZ2+pw4qIaHtVSYtNQlSA9I/2777aAxbCyqta8WctxY6c6ONR49PjhpMkjdu74vp63Dxtt6LB+bdp0mDTxS9i+eev6kSMHH0c8tLa2bdCgUeDYqTY2tp38m8OhdeuX7wradPrkZdi+fv3K9wd3v4iJhpQ9Pb2mT/0KUobwfp90GTlifFxczK/Hf65WrXrrVu2nTJ69avVCiF+rluvQIaO7du1Zzg3vPxB08ND/YAOuCPfz2YAvsrOzN25ede9ecEZGupurR/fuffv1/YyNXM4hFTExzyHNe/dDaJr28Wk4aOBwX18tZt8mofZCaGN5Yh9nisS6ck8np8R+d2CqTJY3JfB/I4asSUh6smvfRIWCGaYiEhvl5GSc+H39wH7z1y272bBB56MnVqS+TYRDN27/euP2sU96zpk+fr9N9ZoX/t6LdAYhIaBpLDIkE3ENoszyt0bkcvm8r6clp7zeuCFo6pQ5r14nzZs/TV7uiKDIJ4+/nj+9SZOPDuw7Nm3q3GfPItesXQLhf5xl5t2dM3shq5zgkFuLlswBGRw9fHbxwtVJSQmbt65mUzAyMjp85HsXF7fz526MHTP53B+nvpwZ6N/54wvnb3bqGLBuw/KMzIxybmDUyAmDPh8OOvz7YjAoB0Lgnl++jFu+bANcy8/Pf8vWNSB+NnI5h1jy8/NnzAwUiURrVm/bsG6XWCT+ZsGXWi2mwsy+Q2iWiebQzDSFSKyrcXL/3f9DLDIaOXiNg51bDXuPz/p+E58QEfaoYGY9hUIW0Gmsay1fyCXNG/eEt0V8QiSEX/v3aEMff5CTqakl2CJPj+ZIl8DjSozJRVyjxAxs7+LmrWuPHoVNnjizSePm/p27gQWoXbvumzcp5ZwSFnrP2Nh46BejIfu2bNEGMtzgwSNLR9u3f5df+84DPh0CZgde55Mmzrx589rjwnJdHU/vPr0/lUgkHTsEwC5EANmIxeJOHbuCdGNeRCMtvsL10NB7c2YtBCMJ1/piyCiwG2Dxyj+kIjb2RWrqm08/GVy3jnft2nUWL1q9dOk6hULx/jdAU6isHlGaFSLLVyBCVy5RKLPVcq5vZlbgBLeu7mhj7Rz94p4qgotTQVnC1MQSPnNymZnckt/EOti7q+I41/RGOiY3m/djNp89e2JqagpGgN2FDLRg/gp7e4dyTmng2zg3N/frb2b8cuzHuPhYyJQgvNLRoqKeeBcW+QCvusw8E48L3/qqK5qZMaVxN7fa7K6JCTNGC4pY6L2Jjn4KYnZ3r60KqVunHlv7KueQCmdnFyg6rl675Icf94WF3SdJEr6O1qsI01qNJCV0px0QQ2ZsfDg4mtUD0zOKXoelSya5eVkUpZBKTVUhEh3PvMQYapqDLSrMxJ7vHzsrK1Mq1c5lCgJb/e3Wq1cv7t6zbeeuTc2atoAKDNR81ONkZmbm5eWppwwSRUwNpGCijBK/IGRZVFFSUpKNjYv91nCtnJzs8g+pkEqlWzbt+f3siWO//rR3386aNZ1HDg8MCOiBKgPN4pEaizPTtTBtWmFhYePu2rhb52LrMJuZldegZCw1I0mRTFZUjsrLz0a6BN41pubcm4tYOcvl+0c3NTWDzERR1Duzr1xRZGahtAZ/UPcICbkFlf7538w4/muxGVvhfQ+fubk5qpAspWxsrG1RZQO2S/1C7LVsbezKP6QOmMGJE2bA1/nvv9tQAVu1elHduvVcXd3R+wGvUe3aeapZG9E6m2iipkOdt2mJHm5NPD2asX/m5tXtbd3KOQXuvno1x+cxoaqQRxE6WTdGBaWga7hybtgSgbQrEXh71YcyWETkI3YX/E5Qe4aynFTCTMSsekmDJUlOfs1u37sXcuv2DdiwtbXr1q3X5EmzoH6fmFSs1QtqL1516z18+EAVwm571K6DKhsoEMJXePI0QhUCtTg3ZVGtnEMq4CuDYJBS8G3a+C1ZvAZuHjyESBvK8BeUIR4XL1OFTFdN7OB9hnfhqXOb8vNzX71+ceb89g3bhyQkPS3/rEYNuoSG/30vlFm0+dI/B1/EhSGdIctSIAp5NjFFHIPpYaBNB4PmzVs5OdXavXvrP9f+vhN8c/OW1a9fJcFLF1zGFuYW4L+GyiTU4FevXWxhYcmeEvbw/pKlc0+fOf72bWr4o7Djvx0GFdVwcITyj52dfXDwzbv3guGU/v0+v3b98q+//gyucAjZuWtj0yYf1fH0QpUBVFSgSHbt2mWo7rdo0QbKWhs3rgRvBLg6oOgFCvn8s2GIWce7zEMq0tPT1q5btitoM9TfILUff9oPNw9ek/e/GcZhoFWv6jrNzeCxZqXopKEQ3GWzp/wkMTLZHDRi7daBUc//+6zfN+90AHTpMKpls74nzm6AyhKYnT7dZyCEdDRLfWL0W5G0KnQhg7fs+rU7KZpatHjO3K+mGJuYfLtqCwSCN3nhwm+hft+5y0eDv+gNPjFHRyf2YQ78bGjPHv2371jf/9MAcDFDwW/Txt3sNHpfDBn93907CxfNysnNASf1mNGTjvxyqG+/zuDLbujbZNHCb1El0aplO98GjRcunn3x0nm49IplGywtraBhasjQPiH/3V6+bD3bUFPOIRVQW5v55fy/Lp4bNrz/8JGfhobeBa+9U01nVBmUOVH8vsXPESn2aOGIhEfE5RgHV+N+kzj33XfNfebkadLp85oIoy9O7IrJy1KMXa6hjlRmPbJRe6vcjDwkSPLy5RxUDmIrrwijV0iizG7VZTqUmnWpfvvP1PjHKU7emuctgIb/DTu+0HjIRGqek6e5eb6GnceUwD2o8liw0r+sQwqFXCTS8AXdXBqOHbaprLOe3UqwtDJCnISuKn2qoR0JWmM1HurRox84xxBnoAmaJLSfeqpJx2ohl1LLEo+lhe3MSYc0HgJPgESiuXmBJCvZ/1vWPTC3IcuTGGlY30ssKq/PXk567rgVnoibVJXhCLNnLsiXaa5Rm5pwy08DDoOyJl0pLyu36mH96E56dHCCe3MNZRh4qVtXN3zhu3LvIfKfWOc6plLOudkKoMueBolf2NhUfouQ7ijrlfWOtrNRi92g5pP+inu9vHRA/MMUkYjuN5G71XFl8RsPhuMK7+43MW5F7dgHiaiqk/g4Nf1Vxpjl79vwbBCI8gYFY3RCOQ3T7xaPWIImr60ddiE6LTEHVVHiQ5PTktInrq2NuA0ehq1/6DL7hb7ndLsiNGWjZ9zDpOfBSajKEXk9PvNN1vjVHgiDKYVyfZoPmG6XZfL62jQle/T3i1dP36IqwYt7rx/+FW1ZXYSVgykLhZxWKCpjcatRi11vnU29dzU1JTbN2NLYoY61qSVHm0TKIS0x63VUWm52nsRY1Gu0k6sPb+atZyZ6Rxi9Uk6vaq1bXVr2qA5/t869eXgzPfoOM78ZKSIIkhAZiZg12lRruBEEM36VUlt2iii2whtNkgTjQlfukMrJ6FGBO0k1co9JgS6WFMGcpDxKEuywSkItpnJHubocu1occ1jZh19MIgW0mVK0gpLLFBBmYW3UsX9Nz6Zc9UmXATPRO8LoFSaT0pVheVS07G4Nf7ARGZL19F5GanI+WDdKhqhCiYJSaLhsoWSZhTEg+6sNESLFlLL5iT1cOJUfyAROLIpGF7iXiIKkCJJiR4MRIlp5boFiCqRDsAfhEHM5ZQjzSRpRIikpNRZXd5B4NTV3qcczzWC4yYe299dtZgZ/CIMRHtwbLIkpG7GUMDISxKT73EFiRBJSzYewePiEVCrOzeLiPMBVGLkcSUy1mXoKw01c65qlJOGpTPVKVqqsXgvNyx1g8fCJDgNtCERdPvwaYfTC6aCXxhYi33bmGo8bbRdXAAAA/UlEQVQSOhrJjNEdB5a9EIlEzbra16qrq/mQMU/vZty/kmpuJRoww6msOFg8vOTopvg3iXnMin9lrHJOK9u7aE2r+TAtCJoKHNAIQGqalJltLNOYvoZUmAuWjqzpNmgNHS5LJ6vxXGjqK7EouHKkRsmJ9kqfqyl9DV+EFJEiMVHD1aTvxPIGFGPx8JicHJSfWdQoVizbEoV5R/3nJQpzmaZTCrYLWp3LO6tY5OLZpyALlwhU6rhkcDnJqm8Vj1HsuiXuHmm6GYTK+sIanxCLlbkIvUe3EyweDKaCYFc1BlNBsHgwmAqCxYPBVBAsHgymgmDxYDAVBIsHg6kg/wcAAP//oMnNZAAAAAZJREFUAwDFaEWJqF9vBgAAAABJRU5ErkJggg==",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count:  1\n",
      "Count:  3\n",
      "==================================================\n",
      "{'messages': [HumanMessage(content='What is 20 * 34?', additional_kwargs={}, response_metadata={}, id='20fb0434-38c3-4dbe-ad5b-009034fab45e'), AIMessage(content='', additional_kwargs={}, response_metadata={'model': 'gpt-oss:20b', 'created_at': '2026-01-27T05:48:41.286031Z', 'done': True, 'done_reason': 'stop', 'total_duration': 60243299541, 'load_duration': 185999125, 'prompt_eval_count': 220, 'prompt_eval_duration': 3548091458, 'eval_count': 41, 'eval_duration': 55824276666, 'logprobs': None, 'model_name': 'gpt-oss:20b', 'model_provider': 'ollama'}, id='lc_run--019bfdfe-5058-7681-8163-cd78fbe87a70-0', tool_calls=[{'name': 'calculate', 'args': {'expression': '20 * 34'}, 'id': '04be7d0d-19b1-43eb-a91f-109da211f793', 'type': 'tool_call'}], invalid_tool_calls=[], usage_metadata={'input_tokens': 220, 'output_tokens': 41, 'total_tokens': 261}), ToolMessage(content='The result of 20 * 34 is 680', name='calculate', id='20c60c88-9c41-40a2-9434-b37a521c613e', tool_call_id='04be7d0d-19b1-43eb-a91f-109da211f793'), AIMessage(content='680', additional_kwargs={}, response_metadata={'model': 'gpt-oss:20b', 'created_at': '2026-01-27T05:49:05.03996Z', 'done': True, 'done_reason': 'stop', 'total_duration': 21883773417, 'load_duration': 1367377958, 'prompt_eval_count': 261, 'prompt_eval_duration': 13385439584, 'eval_count': 5, 'eval_duration': 6439201709, 'logprobs': None, 'model_name': 'gpt-oss:20b', 'model_provider': 'ollama'}, id='lc_run--019bfdff-41fb-7080-8f32-4487e4c08c8d-0', tool_calls=[], invalid_tool_calls=[], usage_metadata={'input_tokens': 261, 'output_tokens': 5, 'total_tokens': 266})], 'iteration_count': 3}\n",
      "\n",
      "Conversation:\n",
      "  [HumanMessage]: What is 20 * 34?, Iteration count: 3\n",
      "  [AIMessage]: [Tool calls: [{'name': 'calculate', 'args': {'expression': '20 * 34'}, 'id': '04be7d0d-19b1-43eb-a91f-109da211f793', 'type': 'tool_call'}]], Iteration count: 3\n",
      "  [ToolMessage]: The result of 20 * 34 is 680, Iteration count: 3\n",
      "  [AIMessage]: 680, Iteration count: 3\n"
     ]
    }
   ],
   "source": [
    "# Test your custom agent\n",
    "llm = ChatOllama(\n",
    "    model=\"gpt-oss:20b\",\n",
    "    temperature=0,  # Deterministic for reproducibility\n",
    ")\n",
    "\n",
    "print(f\"LLM initialized: {llm.model}\")\n",
    "\n",
    "\n",
    "tools = [calculate, get_current_time]\n",
    "\n",
    "llm_with_tools = llm.bind_tools(tools)\n",
    "\n",
    "print(\"Tools defined and bound to LLM:\")\n",
    "for t in tools:\n",
    "    print(f\"  - {t.name}: {t.description[:50]}...\")\n",
    "\n",
    "\n",
    "SYSTEM_PROMPT = \"\"\"You are a helpful assistant that can perform calculations and tell the time.\n",
    "Always use the available tools when appropriate.\n",
    "Be concise in your responses.\"\"\"\n",
    "\n",
    "workflow = StateGraph(AgentStateWithCounter)\n",
    "\n",
    "workflow.add_node(\"custom_agent\", custom_agent_node)\n",
    "workflow.add_node(\"custom_tools\", custom_tool_node)\n",
    "\n",
    "workflow.add_edge(START, \"custom_agent\")\n",
    "\n",
    "workflow.add_conditional_edges(\n",
    "    \"custom_agent\",\n",
    "    custom_should_continue,\n",
    "    {\n",
    "        \"custom_tools\": \"custom_tools\",  # If should_continue returns \"tools\", go to tools node\n",
    "        \"end\": END         # If should_continue returns \"end\", finish\n",
    "    }\n",
    ")\n",
    "\n",
    "workflow.add_edge(\"custom_tools\", \"custom_agent\")\n",
    "\n",
    "custom_agent = workflow.compile()\n",
    "\n",
    "print(\"Custom agent built from scratch!\")\n",
    "\n",
    "# Test the custom agent\n",
    "print(\"Testing custom agent:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "try:\n",
    "    from IPython.display import display, Image\n",
    "    display(Image(custom_agent.get_graph().draw_mermaid_png()))\n",
    "except Exception as e:\n",
    "    print(f\"Could not display graph image: {e}\")\n",
    "    print(\"\\nGraph structure (ASCII):\")\n",
    "    print(custom_agent.get_graph().draw_ascii())\n",
    "\n",
    "\n",
    "response = custom_agent.invoke({\"messages\": [HumanMessage(content=\"What is 20 * 34?\")], \"iteration_count\": 0})\n",
    "\n",
    "print(\"=\" * 50)\n",
    "print(response)\n",
    "\n",
    "print(\"\\nConversation:\")\n",
    "for msg in response[\"messages\"]:\n",
    "    msg_type = type(msg).__name__\n",
    "    content = msg.content if msg.content else f\"[Tool calls: {msg.tool_calls}]\" if hasattr(msg, 'tool_calls') and msg.tool_calls else \"[No content]\"\n",
    "    count = response.get(\"iteration_count\")\n",
    "    print(f\"  [{msg_type}]: {content[:200]}, Iteration count: {count}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-30",
   "metadata": {},
   "source": [
    "---\n",
    "# Breakout Room #2\n",
    "## Agentic RAG with Local Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-31",
   "metadata": {},
   "source": [
    "Now let's build a full **Agentic RAG** system from scratch using our local models!\n",
    "\n",
    "We'll transition from the `aimakerspace` utilities to the **LangChain ecosystem**:\n",
    "\n",
    "| Task | aimakerspace | LangChain |\n",
    "|------|--------------|----------|\n",
    "| Load Documents | `TextFileLoader` | `TextLoader` |\n",
    "| Split Text | `CharacterTextSplitter` | `RecursiveCharacterTextSplitter` |\n",
    "| Embeddings | Custom | `OllamaEmbeddings` |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-32",
   "metadata": {},
   "source": [
    "## Task 5: Loading & Chunking with LangChain\n",
    "\n",
    "Let's use LangChain's document loaders and text splitters.\n",
    "\n",
    "**ðŸ“š Documentation:**\n",
    "- [Document Loaders Conceptual Guide](https://python.langchain.com/docs/concepts/document_loaders/)\n",
    "- [TextLoader Reference](https://python.langchain.com/api_reference/community/document_loaders/langchain_community.document_loaders.text.TextLoader.html)\n",
    "- [RecursiveCharacterTextSplitter](https://python.langchain.com/docs/how_to/recursive_text_splitter/)\n",
    "- [Text Splitters Conceptual Guide](https://python.langchain.com/docs/concepts/text_splitters/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "cell-33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 1 document(s)\n",
      "Total characters: 16,206\n",
      "\n",
      "Document metadata: {'source': 'data/HealthWellnessGuide.txt'}\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.document_loaders import TextLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "# Load the document using LangChain's TextLoader\n",
    "loader = TextLoader(\"data/HealthWellnessGuide.txt\")\n",
    "documents = loader.load()\n",
    "\n",
    "print(f\"Loaded {len(documents)} document(s)\")\n",
    "print(f\"Total characters: {sum(len(doc.page_content) for doc in documents):,}\")\n",
    "print(f\"\\nDocument metadata: {documents[0].metadata}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "cell-34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split into 45 chunks\n",
      "\n",
      "Sample chunk (first 300 chars):\n",
      "--------------------------------------------------\n",
      "The Personal Wellness Guide\n",
      "A Comprehensive Resource for Health and Well-being\n",
      "\n",
      "PART 1: EXERCISE AND MOVEMENT\n",
      "\n",
      "Chapter 1: Understanding Exercise Basics\n",
      "\n",
      "Exercise is one of the most important things you can do for your health. Regular physical activity can improve your brain health, help manage weigh...\n"
     ]
    }
   ],
   "source": [
    "# Split documents using RecursiveCharacterTextSplitter\n",
    "# This is more sophisticated than simple character splitting!\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=500,\n",
    "    chunk_overlap=100,\n",
    "    length_function=len,\n",
    "    # Default separators: [\"\\n\\n\", \"\\n\", \" \", \"\"]\n",
    "    # Tries to keep paragraphs, then sentences, then words together\n",
    ")\n",
    "\n",
    "chunks = text_splitter.split_documents(documents)\n",
    "\n",
    "print(f\"Split into {len(chunks)} chunks\")\n",
    "print(f\"\\nSample chunk (first 300 chars):\")\n",
    "print(\"-\" * 50)\n",
    "print(chunks[0].page_content[:300] + \"...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-35",
   "metadata": {},
   "source": [
    "## Task 6: Setting up Qdrant with Local Embeddings\n",
    "\n",
    "Now we'll use **OllamaEmbeddings** with the `embeddinggemma` model - completely local!\n",
    "\n",
    "**ðŸ“š Documentation:**\n",
    "- [OllamaEmbeddings Reference](https://python.langchain.com/api_reference/ollama/embeddings/langchain_ollama.embeddings.OllamaEmbeddings.html)\n",
    "- [Qdrant Vector Store Integration](https://python.langchain.com/docs/integrations/vectorstores/qdrant/)\n",
    "- [Embedding Models Conceptual Guide](https://python.langchain.com/docs/concepts/embedding_models/)\n",
    "- [EmbeddingGemma Overview (Google)](https://ai.google.dev/gemma/docs/embeddinggemma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "cell-36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding dimension: 768\n",
      "Using local model: embeddinggemma\n"
     ]
    }
   ],
   "source": [
    "from langchain_ollama import OllamaEmbeddings\n",
    "from langchain_qdrant import QdrantVectorStore\n",
    "from qdrant_client import QdrantClient\n",
    "from qdrant_client.http.models import Distance, VectorParams\n",
    "\n",
    "# Initialize local embedding model\n",
    "embedding_model = OllamaEmbeddings(model=\"embeddinggemma\")\n",
    "\n",
    "# Get embedding dimension\n",
    "sample_embedding = embedding_model.embed_query(\"test\")\n",
    "embedding_dim = len(sample_embedding)\n",
    "print(f\"Embedding dimension: {embedding_dim}\")\n",
    "print(f\"Using local model: embeddinggemma\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "cell-37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created collection: wellness_knowledge_base_local\n"
     ]
    }
   ],
   "source": [
    "# Create Qdrant client (in-memory for development)\n",
    "qdrant_client = QdrantClient(\":memory:\")\n",
    "\n",
    "# Create a collection for our wellness documents\n",
    "collection_name = \"wellness_knowledge_base_local\"\n",
    "\n",
    "qdrant_client.create_collection(\n",
    "    collection_name=collection_name,\n",
    "    vectors_config=VectorParams(\n",
    "        size=embedding_dim,\n",
    "        distance=Distance.COSINE\n",
    "    )\n",
    ")\n",
    "\n",
    "print(f\"Created collection: {collection_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "cell-38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding documents to vector store (this may take a moment with local embeddings)...\n",
      "Added 45 documents to vector store\n"
     ]
    }
   ],
   "source": [
    "# Create vector store and add documents\n",
    "vector_store = QdrantVectorStore(\n",
    "    client=qdrant_client,\n",
    "    collection_name=collection_name,\n",
    "    embedding=embedding_model\n",
    ")\n",
    "\n",
    "# Add documents to the vector store\n",
    "print(\"Adding documents to vector store (this may take a moment with local embeddings)...\")\n",
    "vector_store.add_documents(chunks)\n",
    "print(f\"Added {len(chunks)} documents to vector store\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "cell-39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieved documents:\n",
      "\n",
      "--- Document 1 ---\n",
      "Chapter 8: Improving Sleep Quality\n",
      "\n",
      "Sleep hygiene refers to habits and practices that promote consistent, quality sleep.\n",
      "\n",
      "Essential sleep hygiene practices:\n",
      "- Maintain a consistent sleep schedule, eve...\n",
      "\n",
      "--- Document 2 ---\n",
      "Creating an optimal sleep environment:\n",
      "- Temperature: 65-68 degrees Fahrenheit (18-20 Celsius)\n",
      "- Darkness: Use blackout curtains or a sleep mask\n",
      "- Quiet: Consider white noise machines or earplugs\n",
      "- Co...\n",
      "\n",
      "--- Document 3 ---\n",
      "Types of insomnia:\n",
      "- Acute insomnia: Short-term, often triggered by stress or life events\n",
      "- Chronic insomnia: Long-term, occurring at least 3 nights per week for 3 months or more\n",
      "\n",
      "Natural remedies for...\n"
     ]
    }
   ],
   "source": [
    "# Test the retriever\n",
    "retriever = vector_store.as_retriever(search_kwargs={\"k\": 3})\n",
    "\n",
    "test_results = retriever.invoke(\"How can I improve my sleep?\")\n",
    "\n",
    "print(\"Retrieved documents:\")\n",
    "for i, doc in enumerate(test_results, 1):\n",
    "    print(f\"\\n--- Document {i} ---\")\n",
    "    print(doc.page_content[:200] + \"...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-40",
   "metadata": {},
   "source": [
    "## Task 7: Creating a RAG Tool\n",
    "\n",
    "Now let's wrap our retriever as a tool that the agent can use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "cell-41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RAG tool created: search_wellness_knowledge\n"
     ]
    }
   ],
   "source": [
    "@tool\n",
    "def search_wellness_knowledge(query: str) -> str:\n",
    "    \"\"\"Search the wellness knowledge base for information about health, fitness, nutrition, sleep, and mental wellness.\n",
    "    \n",
    "    Use this tool when the user asks questions about:\n",
    "    - Physical health and fitness\n",
    "    - Nutrition and diet\n",
    "    - Sleep and rest\n",
    "    - Mental health and stress management\n",
    "    - General wellness tips\n",
    "    \n",
    "    Args:\n",
    "        query: The search query to find relevant wellness information\n",
    "    \"\"\"\n",
    "    results = retriever.invoke(query)\n",
    "    \n",
    "    if not results:\n",
    "        return \"No relevant information found in the wellness knowledge base.\"\n",
    "    \n",
    "    # Format the results\n",
    "    formatted_results = []\n",
    "    for i, doc in enumerate(results, 1):\n",
    "        formatted_results.append(f\"[Source {i}]:\\n{doc.page_content}\")\n",
    "    \n",
    "    return \"\\n\\n\".join(formatted_results)\n",
    "\n",
    "print(f\"RAG tool created: {search_wellness_knowledge.name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-42",
   "metadata": {},
   "source": [
    "## Task 8: Building Agentic RAG from Scratch\n",
    "\n",
    "Now let's put it all together - a complete agentic RAG system built from scratch!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "cell-43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tools for RAG agent:\n",
      "  - search_wellness_knowledge\n",
      "  - calculate\n",
      "  - get_current_time\n"
     ]
    }
   ],
   "source": [
    "# Define all tools for our RAG agent\n",
    "rag_tools = [search_wellness_knowledge, calculate, get_current_time]\n",
    "\n",
    "# Bind tools to the LLM\n",
    "rag_llm_with_tools = llm.bind_tools(rag_tools)\n",
    "\n",
    "print(\"Tools for RAG agent:\")\n",
    "for t in rag_tools:\n",
    "    print(f\"  - {t.name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "cell-44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RAG agent node defined\n"
     ]
    }
   ],
   "source": [
    "# Define the RAG agent components\n",
    "RAG_SYSTEM_PROMPT = \"\"\"You are a helpful wellness assistant with access to a comprehensive health and wellness knowledge base.\n",
    "\n",
    "Your role is to:\n",
    "1. Answer questions about health, fitness, nutrition, sleep, and mental wellness\n",
    "2. ALWAYS search the knowledge base when the user asks wellness-related questions\n",
    "3. Provide accurate, helpful information based on the retrieved context\n",
    "4. Be supportive and encouraging in your responses\n",
    "5. If you cannot find relevant information, say so honestly\n",
    "\n",
    "Remember: Always cite information from the knowledge base when applicable.\"\"\"\n",
    "\n",
    "def rag_agent_node(state: AgentState):\n",
    "    \"\"\"The RAG agent node - calls the LLM with wellness system prompt.\"\"\"\n",
    "    messages = [SystemMessage(content=RAG_SYSTEM_PROMPT)] + state[\"messages\"]\n",
    "    response = rag_llm_with_tools.invoke(messages)\n",
    "    return {\"messages\": [response]}\n",
    "\n",
    "# Create tool node for RAG tools\n",
    "rag_tool_node = ToolNode(rag_tools)\n",
    "\n",
    "print(\"RAG agent node defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "cell-45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Agentic RAG built from scratch!\n"
     ]
    }
   ],
   "source": [
    "# Build the RAG agent graph\n",
    "rag_workflow = StateGraph(AgentState)\n",
    "\n",
    "# Add nodes\n",
    "rag_workflow.add_node(\"agent\", rag_agent_node)\n",
    "rag_workflow.add_node(\"tools\", rag_tool_node)\n",
    "\n",
    "# Set entry point\n",
    "rag_workflow.add_edge(START, \"agent\")\n",
    "\n",
    "# Add conditional edge\n",
    "rag_workflow.add_conditional_edges(\n",
    "    \"agent\",\n",
    "    should_continue,\n",
    "    {\"tools\": \"tools\", \"end\": END}\n",
    ")\n",
    "\n",
    "# Add edge from tools back to agent\n",
    "rag_workflow.add_edge(\"tools\", \"agent\")\n",
    "\n",
    "# Compile\n",
    "rag_agent = rag_workflow.compile()\n",
    "\n",
    "print(\"Agentic RAG built from scratch!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "cell-46",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Visualize the RAG agent\n",
    "try:\n",
    "    from IPython.display import display, Image\n",
    "    display(Image(rag_agent.get_graph().draw_mermaid_png()))\n",
    "except Exception as e:\n",
    "    print(f\"Could not display graph image: {e}\")\n",
    "    print(\"\\nGraph structure:\")\n",
    "    print(rag_agent.get_graph().draw_ascii())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "cell-47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing Agentic RAG (with local models):\n",
      "==================================================\n",
      "\n",
      "Final Response:\n",
      "==================================================\n",
      "Here are some evidenceâ€‘based tips that can help you fall asleep faster, stay asleep longer, and wake up feeling refreshed:\n",
      "\n",
      "| Tip | Why it works | How to implement it |\n",
      "|-----|--------------|---------------------|\n",
      "| **Keep a consistent sleep schedule** | Your bodyâ€™s internal clock (circadian rhythm) thrives on regularity. | Go to bed and wake up at the same time every dayâ€”even on weekends. |\n",
      "| **Create a calming preâ€‘bed routine** | Activities that lower heart rate and relax the mind signal the body that itâ€™s time to wind down. | Read a book, do gentle stretching, or take a warm bath 30â€“60â€¯min before bed. |\n",
      "| **Optimize your bedroom environment** | Light, noise, temperature, and comfort all influence sleep quality. | â€¢ Keep the room **cool** (65â€“68â€¯Â°F / 18â€“20â€¯Â°C). <br>â€¢ Make it **dark** with blackout curtains or a sleep mask. <br>â€¢ Reduce noise with a whiteâ€‘noise machine or earplugs. <br>â€¢ Use a supportive mattress and pillows. |\n",
      "| **Limit screen time before bed** | Blue light suppresses melatonin, the hormone that signals sleep. | Turn off TVs, phones, and computers at least 1â€“2â€¯h before bedtime. |\n",
      "| **Watch caffeine and alcohol** | Caffeine can stay in your system for 6â€“8â€¯h; alcohol disrupts REM sleep. | Avoid caffeine after 2â€¯p.m. and limit alcohol to a small amount earlier in the evening. |\n",
      "| **Mind what you eat and drink** | Heavy meals or large fluids can cause discomfort or nighttime trips to the bathroom. | Finish eating at least 2â€“3â€¯h before bed and stay hydrated earlier in the day. |\n",
      "| **Exercise regularly, but not too close to bedtime** | Physical activity promotes deeper sleep, but exercising right before bed can raise core temperature and adrenaline. | Aim for at least 30â€¯min of moderate activity most days, finishing 3â€“4â€¯h before sleep. |\n",
      "| **Use relaxation techniques if youâ€™re stressed** | Stress hormones keep the brain alert. | Try deep breathing, progressive muscle relaxation, or guided imagery. |\n",
      "| **Limit naps to 20â€“30â€¯min** | Long or late naps can interfere with nighttime sleep. | If you nap, keep it short and earlier in the day. |\n",
      "\n",
      "**Quick reference**  \n",
      "- **Ideal temperature:** 65â€“68â€¯Â°F (18â€“20â€¯Â°C)  \n",
      "- **Recommended sleep duration for adults:** 7â€“9â€¯h per night  \n",
      "- **Sleep cycle length:** ~90â€¯min (REM + nonâ€‘REM)\n",
      "\n",
      "These strategies are drawn from the wellness knowledge baseâ€™s chapters on sleep hygiene and insomnia management. Give them a try, and youâ€™ll likely notice a smoother, more restorative sleep pattern. Sweet dreams! ðŸŒ™\n"
     ]
    }
   ],
   "source": [
    "# Test the RAG agent\n",
    "print(\"Testing Agentic RAG (with local models):\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "response = rag_agent.invoke({\n",
    "    \"messages\": [HumanMessage(content=\"What are some tips for better sleep?\")]\n",
    "})\n",
    "\n",
    "print(\"\\nFinal Response:\")\n",
    "print(\"=\" * 50)\n",
    "print(response[\"messages\"][-1].content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "cell-48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing with complex query:\n",
      "==================================================\n",
      "\n",
      "Final Response:\n",
      "==================================================\n",
      "Iâ€™m sorry youâ€™re feeling so stressed and having trouble sleeping. Below are some evidenceâ€‘based strategies that can help you calm your mind, improve sleep quality, and manage stress. Iâ€™ve pulled these from the wellness knowledge base to give you the most reliable guidance.\n",
      "\n",
      "---\n",
      "\n",
      "## 1. Quickâ€‘Start Stressâ€‘Reduction Techniques\n",
      "\n",
      "| Technique | How it Helps | Practical Tips |\n",
      "|-----------|--------------|----------------|\n",
      "| **Deepâ€‘breathing / diaphragmatic breathing** | Activates the parasympathetic nervous system, lowering heart rate and cortisol. | 4â€‘7â€‘8 method: inhale 4â€¯s, hold 7â€¯s, exhale 8â€¯s. Do 3â€“5 cycles before bed. |\n",
      "| **Progressive muscle relaxation** | Releases muscle tension that often builds up with stress. | Tense each muscle group for 5â€¯s, then release for 15â€¯s. Work from toes to head. |\n",
      "| **Mindfulness or guided imagery** | Shifts focus away from racing thoughts. | Use a 5â€‘minute guided meditation app (e.g., Insight Timer, Calm). |\n",
      "| **Limit screen time 1â€¯h before bed** | Blue light suppresses melatonin. | Replace with a book, gentle stretching, or a warm shower. |\n",
      "\n",
      "> *Source: Sleep hygiene and stress management guidelines in the wellness knowledge base.*\n",
      "\n",
      "---\n",
      "\n",
      "## 2. Sleepâ€‘Friendly Environment\n",
      "\n",
      "| Element | Why it matters | Quick Fix |\n",
      "|---------|----------------|-----------|\n",
      "| **Consistent bedtime & wake time** | Regulates circadian rhythm. | Aim for the same 7â€“9â€¯h window every day, even weekends. |\n",
      "| **Cool, dark room** | Promotes deeper sleep stages. | Use blackout curtains, a fan, or a whiteâ€‘noise machine. |\n",
      "| **Comfortable mattress & pillows** | Reduces physical discomfort that can wake you. | Replace if you feel aches or stiffness. |\n",
      "| **Limit caffeine & alcohol** | Both can fragment sleep. | Avoid caffeine after 2â€¯pm; limit alcohol to 1â€“2 drinks at most. |\n",
      "\n",
      "> *Source: Sleep cycle and recommended duration information.*\n",
      "\n",
      "---\n",
      "\n",
      "## 3. Lifestyle Tweaks That Reduce Stress & Improve Sleep\n",
      "\n",
      "| Habit | Impact | How to Start |\n",
      "|-------|--------|--------------|\n",
      "| **Regular moderate exercise** | Enhances sleep quality and lowers cortisol. | 30â€¯min brisk walk or light yoga 3â€“4â€¯days/week. |\n",
      "| **Balanced nutrition** | Stable blood sugar keeps you from nighttime awakenings. | Include protein, healthy fats, and complex carbs at dinner. |\n",
      "| **Hydration** | Dehydration can cause nighttime awakenings. | Aim for 1.5â€“2â€¯L/day, but reduce fluids 2â€¯h before bed. |\n",
      "| **Journaling** | Offloads worries before sleep. | Write 5â€“10 minutes of thoughts or gratitude. |\n",
      "\n",
      "> *Source: Immuneâ€‘boosting strategies and general wellness tips.*\n",
      "\n",
      "---\n",
      "\n",
      "## 4. If 6â€¯Hours a Night Is All You Can Get\n",
      "\n",
      "You mentioned sleeping **6â€¯hours per night for a week**. Letâ€™s calculate the total:\n",
      "\n",
      "\\[\n",
      "6 \\text{ hours/night} \\times 7 \\text{ nights} = 42 \\text{ hours}\n",
      "\\]\n",
      "\n",
      "So youâ€™ll have accumulated **42 hours of sleep** over that week. While thatâ€™s a decent amount of total sleep, itâ€™s still below the 7â€“9â€¯hour recommendation for adults. If possible, try to add an extra 30â€“60â€¯minutes each night or take a short nap (20â€“30â€¯min) during the day to help catch up.\n",
      "\n",
      "---\n",
      "\n",
      "## 5. A Gentle Plan for the Next 3 Days\n",
      "\n",
      "1. **Day 1 â€“ Bedtime Routine**  \n",
      "   - 9:00â€¯pm: Light stretching + 4â€‘7â€‘8 breathing.  \n",
      "   - 9:30â€¯pm: Turn off screens, dim lights, read a book.  \n",
      "   - 10:00â€¯pm: Sleep.\n",
      "\n",
      "2. **Day 2 â€“ Exercise & Nutrition**  \n",
      "   - 7:00â€¯am: 30â€‘min walk.  \n",
      "   - 7:30â€¯pm: Dinner with protein + veggies + complex carb.  \n",
      "   - 10:00â€¯pm: Sleep.\n",
      "\n",
      "3. **Day 3 â€“ Mindfulness & Journaling**  \n",
      "   - 8:30â€¯pm: 5â€‘minute guided meditation.  \n",
      "   - 9:00â€¯pm: Write 3 things youâ€™re grateful for.  \n",
      "   - 10:00â€¯pm: Sleep.\n",
      "\n",
      "Feel free to adjust the times to fit your schedule. The key is consistency and giving your body a predictable rhythm.\n",
      "\n",
      "---\n",
      "\n",
      "### Youâ€™re Not Alone\n",
      "\n",
      "Stress and sleep issues are common, and small, consistent changes can make a big difference. If you find that your sleep or stress levels donâ€™t improve after a few weeks, consider speaking with a healthcare professional or a sleep specialist. They can help rule out underlying conditions and tailor a plan just for you.\n",
      "\n",
      "Take care, and remember that each small step is progress. ðŸŒ±\n"
     ]
    }
   ],
   "source": [
    "# Test with a complex query requiring both RAG and calculation\n",
    "print(\"Testing with complex query:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "response = rag_agent.invoke({\n",
    "    \"messages\": [HumanMessage(\n",
    "        content=\"I'm stressed and sleeping poorly. What should I do? Also, if I sleep 6 hours a night for a week, how many total hours is that?\"\n",
    "    )]\n",
    "})\n",
    "\n",
    "print(\"\\nFinal Response:\")\n",
    "print(\"=\" * 50)\n",
    "print(response[\"messages\"][-1].content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "cell-49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing agent decision-making (should NOT use RAG):\n",
      "==================================================\n",
      "\n",
      "Final Response:\n",
      "125â€¯Ã—â€¯8â€¯=â€¯**1,000**.\n"
     ]
    }
   ],
   "source": [
    "# Test that the agent knows when NOT to use RAG\n",
    "print(\"Testing agent decision-making (should NOT use RAG):\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "response = rag_agent.invoke({\n",
    "    \"messages\": [HumanMessage(content=\"What is 125 * 8?\")]\n",
    "})\n",
    "\n",
    "print(\"\\nFinal Response:\")\n",
    "print(response[\"messages\"][-1].content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-52",
   "metadata": {},
   "source": [
    "---\n",
    "## â“ Question #3:\n",
    "\n",
    "Compare the experience of building an agent from scratch with LangGraph versus using `create_agent` from Session 3. What are the trade-offs between control and convenience? When would you choose one approach over the other?\n",
    "\n",
    "##### Answer:\n",
    "Trade off between control and convenience:\n",
    "1. create_agent includes recursion limits, multiple call handling, schema enforcement, message management\n",
    "2. With custom agent, the routing logic can be modified as per requirements, special error handling per tool, parallel tool execution, custom retry logic, observability hooks\n",
    "\n",
    "When to choose create_agent:\n",
    "1. When you need a working agent quickly\n",
    "2. Standard ReAct behavior is sufficient\n",
    "3. Need built-in production features\n",
    "4. prototyping or learning \n",
    "\n",
    "When to build agent from scratch:\n",
    "1. need custom routing or state management\n",
    "2. need fine grained control over each step\n",
    "3. building a non standard agent architecture\n",
    "4. need observability or custom error handling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-53",
   "metadata": {},
   "source": [
    "## â“ Question #4:\n",
    "\n",
    "We used local models (gpt-oss:20b and embeddinggemma) instead of cloud APIs. What are the advantages and disadvantages of this approach? \n",
    "\n",
    "##### Answer:\n",
    "Advantages of local models:\n",
    "1. Data stays local. Useful for sensitive or proprietary data, no risk of data logged or used for training\n",
    "2. no cost for API calls. Predictable cost, scales with usage without increase in cost\n",
    "3. No API rate limiting ot throttling \n",
    "4. Works without interet\n",
    "\n",
    "Disadvantages of local models:\n",
    "1. Requires significant hardware, Need GPU for acceptable performance, Higher upfront cost for hardware\n",
    "2. Slower than optimized cloud APIs, may have lower quality than cloud models, limited by local compute resources\n",
    "3. Initial setup complexity, model update require manual management, need someone to troubelshoot setup in case of issues\n",
    "4. harder to scale horizontaly, less efficient for bursty workloads"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-54",
   "metadata": {},
   "source": [
    "---\n",
    "## ðŸ—ï¸ Activity #2: Extend the Agent with Memory\n",
    "\n",
    "LangGraph supports **checkpointing** which enables conversation memory across invocations.\n",
    "\n",
    "Your task: Add memory to the RAG agent so it can:\n",
    "1. Remember previous questions in the conversation\n",
    "2. Reference past context when answering new questions\n",
    "3. Build on previous answers\n",
    "\n",
    "Hint: Use `MemorySaver` from `langgraph.checkpoint.memory` and pass a `thread_id` in the config.\n",
    "\n",
    "**ðŸ“š Documentation:**\n",
    "- [LangGraph Persistence & Memory](https://langchain-ai.github.io/langgraph/concepts/persistence/)\n",
    "- [How to add memory to your graph](https://langchain-ai.github.io/langgraph/how-tos/persistence/)\n",
    "- [MemorySaver Reference](https://langchain-ai.github.io/langgraph/reference/checkpoints/#langgraph.checkpoint.memory.MemorySaver)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "cell-55",
   "metadata": {},
   "outputs": [],
   "source": [
    "### YOUR CODE HERE ###\n",
    "\n",
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "\n",
    "# Create a memory saver\n",
    "memory = MemorySaver()\n",
    "\n",
    "# Recompile the agent with checkpointing\n",
    "rag_agent_with_memory = rag_workflow.compile(checkpointer=memory)\n",
    "\n",
    "# Test with a conversation that requires memory\n",
    "# Use config={\"configurable\": {\"thread_id\": \"conversation-1\"}}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "cell-56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "First Response:\n",
      "==================================================\n",
      "The result ofâ€¯24â€¯Ã—â€¯10â€¯isâ€¯**240**.\n",
      "\n",
      "Second Response:\n",
      "==================================================\n",
      "It was just a quick multiplication: you multiplied 24 by 10, which gives 240. If you had something else in mindâ€”like a specific topic or a different calculationâ€”just let me know!\n"
     ]
    }
   ],
   "source": [
    "# Test your memory-enabled agent with a multi-turn conversation\n",
    "response = rag_agent_with_memory.invoke(\n",
    "    {\n",
    "        \"messages\": [HumanMessage(content=\"what is 24*10\")]\n",
    "    },\n",
    "    config={\"configurable\": {\"thread_id\": \"conversation-1\"}}\n",
    ")\n",
    "\n",
    "print(\"\\nFirst Response:\")\n",
    "print(\"=\" * 50)\n",
    "print(response[\"messages\"][-1].content)\n",
    "\n",
    "response = rag_agent_with_memory.invoke(\n",
    "    {\n",
    "        \"messages\": [HumanMessage(content=\"what was it about?\")]\n",
    "    },\n",
    "    config={\"configurable\": {\"thread_id\": \"conversation-1\"}}\n",
    ")\n",
    "\n",
    "print(\"\\nSecond Response:\")\n",
    "print(\"=\" * 50)\n",
    "print(response[\"messages\"][-1].content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-57",
   "metadata": {},
   "source": [
    "---\n",
    "## Summary\n",
    "\n",
    "In this session, we:\n",
    "\n",
    "1. **Built agents from scratch** using LangGraph's low-level primitives (StateGraph, nodes, edges)\n",
    "2. **Used local open-source models** with Ollama (gpt-oss:20b + embeddinggemma)\n",
    "3. **Transitioned to LangChain** for document loading and text splitting\n",
    "4. **Created an Agentic RAG system** that intelligently decides when to retrieve information\n",
    "\n",
    "### Key Takeaways:\n",
    "\n",
    "- **StateGraph** gives you full control over agent architecture\n",
    "- **Conditional edges** enable dynamic routing based on LLM decisions\n",
    "- **Local models** provide privacy and cost savings, with trade-offs in performance\n",
    "- **LangSmith** provides crucial visibility regardless of where your models run\n",
    "\n",
    "### What's Next?\n",
    "\n",
    "Now that you understand the fundamentals, you can:\n",
    "- Add more sophisticated routing logic\n",
    "- Implement human-in-the-loop patterns\n",
    "- Build multi-agent systems\n",
    "- Deploy to production with LangGraph Platform\n",
    "\n",
    "**ðŸ“š Further Reading:**\n",
    "- [LangGraph How-To Guides](https://langchain-ai.github.io/langgraph/how-tos/)\n",
    "- [Human-in-the-Loop Patterns](https://langchain-ai.github.io/langgraph/how-tos/human_in_the_loop/)\n",
    "- [Multi-Agent Architectures](https://langchain-ai.github.io/langgraph/concepts/multi_agent/)\n",
    "- [LangGraph Platform](https://langchain-ai.github.io/langgraph/concepts/langgraph_platform/)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
